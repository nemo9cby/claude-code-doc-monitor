<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>test-and-evaluate/define-success - Diff Report</title>
    <link rel="stylesheet" href="../../css/diff.css">
    <style>
        :root {
            --bg-color: #1a1a2e;
            --card-bg: #16213e;
            --text-color: #eee;
            --accent: #0f3460;
            --add-bg: #1a4d1a;
            --del-bg: #4d1a1a;
            --add-text: #4ade80;
            --del-text: #f87171;
        }
        @media (prefers-color-scheme: light) {
            :root {
                --bg-color: #f5f5f5;
                --card-bg: #fff;
                --text-color: #333;
                --accent: #e0e0e0;
                --add-bg: #d4edda;
                --del-bg: #f8d7da;
                --add-text: #155724;
                --del-text: #721c24;
            }
        }
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: var(--bg-color);
            color: var(--text-color);
            line-height: 1.6;
            padding: 2rem;
        }
        .container { max-width: 1200px; margin: 0 auto; }
        header { margin-bottom: 2rem; }
        h1 { font-size: 1.5rem; margin-bottom: 0.5rem; }
        .meta { color: #888; font-size: 0.9rem; }
        .summary {
            background: var(--card-bg);
            padding: 1rem;
            border-radius: 8px;
            margin-bottom: 2rem;
            display: flex;
            gap: 2rem;
        }
        .stat { display: flex; align-items: center; gap: 0.5rem; }
        .stat.added { color: var(--add-text); }
        .stat.removed { color: var(--del-text); }
        .analysis {
            background: var(--card-bg);
            padding: 1.5rem;
            border-radius: 8px;
            margin-bottom: 2rem;
            border-left: 4px solid #8b5cf6;
        }
        .analysis-header {
            font-weight: 600;
            margin-bottom: 0.75rem;
            color: #8b5cf6;
        }
        .analysis-content {
            white-space: pre-wrap;
            font-size: 0.95rem;
            line-height: 1.7;
        }
        .diff-container {
            background: var(--card-bg);
            border-radius: 8px;
            overflow: hidden;
        }
        .diff-header {
            background: var(--accent);
            padding: 0.75rem 1rem;
            font-weight: 600;
        }
        .diff-content {
            padding: 1rem;
            overflow-x: auto;
        }
        .diff-content ins {
            background: var(--add-bg);
            color: var(--add-text);
            text-decoration: none;
            padding: 0.1em 0.2em;
            border-radius: 2px;
        }
        .diff-content del {
            background: var(--del-bg);
            color: var(--del-text);
            text-decoration: line-through;
            padding: 0.1em 0.2em;
            border-radius: 2px;
        }
        pre {
            background: var(--accent);
            padding: 1rem;
            border-radius: 8px;
            overflow-x: auto;
            font-size: 0.85rem;
            margin-top: 2rem;
        }
        a { color: #60a5fa; }
        .back-link { margin-bottom: 1rem; display: inline-block; }
    </style>
</head>
<body>
    <div class="container">
        <a href="index.html" class="back-link">&larr; Back to daily report</a>

        <header>
            <h1>test-and-evaluate/define-success.md</h1>
            <p class="meta">Changed on 2026-01-06 14:35:17 UTC</p>
        </header>

        <div class="summary">
            <div class="stat added">
                <span>+68</span> lines added
            </div>
            <div class="stat removed">
                <span>-0</span> lines removed
            </div>
        </div>

        
        <div class="analysis">
            <div class="analysis-header">ðŸ¤– AI Analysis</div>
            <div class="analysis-content"># Documentation Change Analysis: `define-success.md`

## Summary
The documentation for defining success criteria in LLM-based applications has been significantly expanded, introducing comprehensive guidelines and examples to help developers establish clear, measurable, and relevant metrics for evaluating their applications.

## Key Changes
- **Introduction of Success Criteria**: Added a section emphasizing the importance of defining success criteria for LLM applications.
- **Detailed Guidelines**: Provided specific attributes of good success criteria: Specific, Measurable, Achievable, and Relevant (SMART).
- **Quantitative and Qualitative Metrics**: Included examples of both quantitative metrics (e.g., F1 score, accuracy) and qualitative methods (e.g., Likert scales).
- **Common Success Criteria**: Listed various success criteria categories such as Task Fidelity, Consistency, Relevance, and Privacy Preservation, with explanations for each.
- **Next Steps**: Added actionable links for brainstorming criteria and designing evaluations, enhancing usability.

## Impact
**Medium**: This change improves the documentation significantly by providing developers with structured guidance on defining success metrics, which is crucial for effective application evaluation and optimization. It enhances clarity and usability, helping developers to align their projects with industry standards.</div>
        </div>
        

        <div class="diff-container">
            <div class="diff-header">Visual Diff</div>
            <div class="diff-content"><ins style="background:#e6ffe6;"># Define your success criteria&para;<br>&para;<br>---&para;<br>&para;<br>Building a successful LLM-based application starts with clearly defining your success criteria. How will you know when your application is good enough to publish?&para;<br>&para;<br>Having clear success criteria ensures that your prompt engineering &amp; optimization efforts are focused on achieving specific, measurable goals.&para;<br>&para;<br>***&para;<br>&para;<br>## Building strong criteria&para;<br>&para;<br>Good success criteria are:&para;<br>- **Specific**: Clearly define what you want to achieve. Instead of "good performance," specify "accurate sentiment classification."&para;<br>- **Measurable**: Use quantitative metrics or well-defined qualitative scales. Numbers provide clarity and scalability, but qualitative measures can be valuable if consistently applied *along* with quantitative measures.&para;<br>    - Even "hazy" topics such as ethics and safety can be quantified:&para;<br>        |      | Safety criteria                |&para;<br>        | ---- | ------------------------------ |&para;<br>        | Bad  | Safe outputs                   |&para;<br>        | Good | Less than 0.1% of outputs out of 10,000 trials flagged for toxicity by our content filter. | &para;<br>    &lt;section title="Example metrics and measurement methods"&gt;&para;<br>&para;<br>        **Quantitative metrics**:&para;<br>            - Task-specific: F1 score, BLEU score, perplexity&para;<br>            - Generic: Accuracy, precision, recall&para;<br>            - Operational: Response time (ms), uptime (%)&para;<br>&para;<br>        **Quantitative methods**:&para;<br>            - A/B testing: Compare performance against a baseline model or earlier version.&para;<br>            - User feedback: Implicit measures like task completion rates.&para;<br>            - Edge case analysis: Percentage of edge cases handled without errors.&para;<br>&para;<br>        **Qualitative scales**:&para;<br>            - Likert scales: "Rate coherence from 1 (nonsensical) to 5 (perfectly logical)"&para;<br>            - Expert rubrics: Linguists rating translation quality on defined criteria        &para;<br>    &para;<br>&lt;/section&gt;&para;<br>- **Achievable**: Base your targets on industry benchmarks, prior experiments, AI research, or expert knowledge. Your success metrics should not be unrealistic to current frontier model capabilities.&para;<br>- **Relevant**: Align your criteria with your application's purpose and user needs. Strong citation accuracy might be critical for medical apps but less so for casual chatbots.&para;<br>&para;<br>&lt;section title="Example task fidelity criteria for sentiment analysis"&gt;&para;<br>&para;<br>    |      | Criteria                                                     |&para;<br>    | ---- | ------------------------------------------------------------ |&para;<br>    | Bad  | The model should classify sentiments well                    |&para;<br>    | Good | Our sentiment analysis model should achieve an F1 score of at least 0.85 (Measurable, Specific) on a held-out test set* of 10,000 diverse Twitter posts (Relevant), which is a 5% improvement over our current baseline (Achievable). |&para;<br>&para;<br>    **More on held-out test sets in the next section*&para;<br>&para;<br>&lt;/section&gt;&para;<br>&para;<br>***&para;<br>&para;<br>## Common success criteria to consider&para;<br>&para;<br>Here are some criteria that might be important for your use case. This list is non-exhaustive.&para;<br>&para;<br>  &lt;section title="Task fidelity"&gt;&para;<br>&para;<br>    How well does the model need to perform on the task? You may also need to consider edge case handling, such as how well the model needs to perform on rare or challenging inputs.&para;<br>  &para;<br>&lt;/section&gt;&para;<br>  &lt;section title="Consistency"&gt;&para;<br>&para;<br>    How similar does the model's responses need to be for similar types of input? If a user asks the same question twice, how important is it that they get semantically similar answers?&para;<br>  &para;<br>&lt;/section&gt;&para;<br>  &lt;section title="Relevance and coherence"&gt;&para;<br>&para;<br>    How well does the model directly address the user's questions or instructions? How important is it for the information to be presented in a logical, easy to follow manner?&para;<br>  &para;<br>&lt;/section&gt;&para;<br>  &lt;section title="Tone and style"&gt;&para;<br>&para;<br>    How well does the model's output style match expectations? How appropriate is its language for the target audience?&para;<br>  &para;<br>&lt;/section&gt;&para;<br>  &lt;section title="Privacy preservation"&gt;&para;<br>&para;<br>    What is a successful metric for how the model handles personal or sensitive information? Can it follow instructions not to use or share certain details?&para;<br>  &para;<br>&lt;/section&gt;&para;<br>  &lt;section title="Context utilization"&gt;&para;<br>&para;<br>    How effectively does the model use provided context? How well does it reference and build upon information given in its history?&para;<br>  &para;<br>&lt;/section&gt;&para;<br>  &lt;section title="Latency"&gt;&para;<br>&para;<br>    What is the acceptable response time for the model? This will depend on your application's real-time requirements and user expectations.&para;<br>  &para;<br>&lt;/section&gt;&para;<br>  &lt;section title="Price"&gt;&para;<br>&para;<br>    What is your budget for running the model? Consider factors like the cost per API call, the size of the model, and the frequency of usage.&para;<br>  &para;<br>&lt;/section&gt;&para;<br>&para;<br>Most use cases will need multidimensional evaluation along several success criteria.&para;<br>&para;<br>&lt;section title="Example multidimensional criteria for sentiment analysis"&gt;&para;<br>&para;<br>    |      | Criteria                                                     |&para;<br>    | ---- | ------------------------------------------------------------ |&para;<br>    | Bad  | The model should classify sentiments well                    |&para;<br>    | Good | On a held-out test set of 10,000 diverse Twitter posts, our sentiment analysis model should achieve:&lt;br/&gt;- an F1 score of at least 0.85&lt;br/&gt;- 99.5% of outputs are non-toxic&lt;br/&gt;- 90% of errors are would cause inconvenience, not egregious error*&lt;br/&gt;- 95% response time &lt; 200ms |&para;<br>&para;<br>    **In reality, we would also define what "inconvenience" and "egregious" means.*&para;<br>&para;<br>&lt;/section&gt;&para;<br>&para;<br>***&para;<br>&para;<br>## Next steps&para;<br>&para;<br>&lt;CardGroup cols={2}&gt;&para;<br>  &lt;Card title="Brainstorm criteria" icon="link" href="https://claude.ai/"&gt;&para;<br>    Brainstorm success criteria for your use case with Claude on claude.ai.&lt;br/&gt;&lt;br/&gt;**Tip**: Drop this page into the chat as guidance for Claude!&para;<br>  &lt;/Card&gt;&para;<br>  &lt;Card title="Design evaluations" icon="link" href="/docs/en/build-with-claude/prompt-engineering/be-clear-and-direct"&gt;&para;<br>    Learn to build strong test sets to gauge Claude's performance against your criteria.&para;<br>  &lt;/Card&gt;&para;<br>&lt;/CardGroup&gt;</ins></div>
        </div>

        <h2 style="margin-top: 2rem; margin-bottom: 1rem;">Unified Diff</h2>
        <pre><code>--- a/test-and-evaluate/define-success.md
+++ b/test-and-evaluate/define-success.md
@@ -0,0 +1,123 @@
+# Define your success criteria
+
+---
+
+Building a successful LLM-based application starts with clearly defining your success criteria. How will you know when your application is good enough to publish?
+
+Having clear success criteria ensures that your prompt engineering &amp; optimization efforts are focused on achieving specific, measurable goals.
+
+***
+
+## Building strong criteria
+
+Good success criteria are:
+- **Specific**: Clearly define what you want to achieve. Instead of &#34;good performance,&#34; specify &#34;accurate sentiment classification.&#34;
+- **Measurable**: Use quantitative metrics or well-defined qualitative scales. Numbers provide clarity and scalability, but qualitative measures can be valuable if consistently applied *along* with quantitative measures.
+    - Even &#34;hazy&#34; topics such as ethics and safety can be quantified:
+        |      | Safety criteria                |
+        | ---- | ------------------------------ |
+        | Bad  | Safe outputs                   |
+        | Good | Less than 0.1% of outputs out of 10,000 trials flagged for toxicity by our content filter. | 
+    &lt;section title=&#34;Example metrics and measurement methods&#34;&gt;
+
+        **Quantitative metrics**:
+            - Task-specific: F1 score, BLEU score, perplexity
+            - Generic: Accuracy, precision, recall
+            - Operational: Response time (ms), uptime (%)
+
+        **Quantitative methods**:
+            - A/B testing: Compare performance against a baseline model or earlier version.
+            - User feedback: Implicit measures like task completion rates.
+            - Edge case analysis: Percentage of edge cases handled without errors.
+
+        **Qualitative scales**:
+            - Likert scales: &#34;Rate coherence from 1 (nonsensical) to 5 (perfectly logical)&#34;
+            - Expert rubrics: Linguists rating translation quality on defined criteria        
+    
+&lt;/section&gt;
+- **Achievable**: Base your targets on industry benchmarks, prior experiments, AI research, or expert knowledge. Your success metrics should not be unrealistic to current frontier model capabilities.
+- **Relevant**: Align your criteria with your application&#39;s purpose and user needs. Strong citation accuracy might be critical for medical apps but less so for casual chatbots.
+
+&lt;section title=&#34;Example task fidelity criteria for sentiment analysis&#34;&gt;
+
+    |      | Criteria                                                     |
+    | ---- | ------------------------------------------------------------ |
+    | Bad  | The model should classify sentiments well                    |
+    | Good | Our sentiment analysis model should achieve an F1 score of at least 0.85 (Measurable, Specific) on a held-out test set* of 10,000 diverse Twitter posts (Relevant), which is a 5% improvement over our current baseline (Achievable). |
+
+    **More on held-out test sets in the next section*
+
+&lt;/section&gt;
+
+***
+
+## Common success criteria to consider
+
+Here are some criteria that might be important for your use case. This list is non-exhaustive.
+
+  &lt;section title=&#34;Task fidelity&#34;&gt;
+
+    How well does the model need to perform on the task? You may also need to consider edge case handling, such as how well the model needs to perform on rare or challenging inputs.
+  
+&lt;/section&gt;
+  &lt;section title=&#34;Consistency&#34;&gt;
+
+    How similar does the model&#39;s responses need to be for similar types of input? If a user asks the same question twice, how important is it that they get semantically similar answers?
+  
+&lt;/section&gt;
+  &lt;section title=&#34;Relevance and coherence&#34;&gt;
+
+    How well does the model directly address the user&#39;s questions or instructions? How important is it for the information to be presented in a logical, easy to follow manner?
+  
+&lt;/section&gt;
+  &lt;section title=&#34;Tone and style&#34;&gt;
+
+    How well does the model&#39;s output style match expectations? How appropriate is its language for the target audience?
+  
+&lt;/section&gt;
+  &lt;section title=&#34;Privacy preservation&#34;&gt;
+
+    What is a successful metric for how the model handles personal or sensitive information? Can it follow instructions not to use or share certain details?
+  
+&lt;/section&gt;
+  &lt;section title=&#34;Context utilization&#34;&gt;
+
+    How effectively does the model use provided context? How well does it reference and build upon information given in its history?
+  
+&lt;/section&gt;
+  &lt;section title=&#34;Latency&#34;&gt;
+
+    What is the acceptable response time for the model? This will depend on your application&#39;s real-time requirements and user expectations.
+  
+&lt;/section&gt;
+  &lt;section title=&#34;Price&#34;&gt;
+
+    What is your budget for running the model? Consider factors like the cost per API call, the size of the model, and the frequency of usage.
+  
+&lt;/section&gt;
+
+Most use cases will need multidimensional evaluation along several success criteria.
+
+&lt;section title=&#34;Example multidimensional criteria for sentiment analysis&#34;&gt;
+
+    |      | Criteria                                                     |
+    | ---- | ------------------------------------------------------------ |
+    | Bad  | The model should classify sentiments well                    |
+    | Good | On a held-out test set of 10,000 diverse Twitter posts, our sentiment analysis model should achieve:&lt;br/&gt;- an F1 score of at least 0.85&lt;br/&gt;- 99.5% of outputs are non-toxic&lt;br/&gt;- 90% of errors are would cause inconvenience, not egregious error*&lt;br/&gt;- 95% response time &lt; 200ms |
+
+    **In reality, we would also define what &#34;inconvenience&#34; and &#34;egregious&#34; means.*
+
+&lt;/section&gt;
+
+***
+
+## Next steps
+
+&lt;CardGroup cols={2}&gt;
+  &lt;Card title=&#34;Brainstorm criteria&#34; icon=&#34;link&#34; href=&#34;https://claude.ai/&#34;&gt;
+    Brainstorm success criteria for your use case with Claude on claude.ai.&lt;br/&gt;&lt;br/&gt;**Tip**: Drop this page into the chat as guidance for Claude!
+  &lt;/Card&gt;
+  &lt;Card title=&#34;Design evaluations&#34; icon=&#34;link&#34; href=&#34;/docs/en/build-with-claude/prompt-engineering/be-clear-and-direct&#34;&gt;
+    Learn to build strong test sets to gauge Claude&#39;s performance against your criteria.
+  &lt;/Card&gt;
+&lt;/CardGroup&gt;</code></pre>
    </div>
</body>
</html>