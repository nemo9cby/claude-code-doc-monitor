<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>build-with-claude/context-editing - Diff Report</title>
    <link rel="stylesheet" href="../../css/diff.css">
    <style>
        :root {
            --bg-color: #1a1a2e;
            --card-bg: #16213e;
            --text-color: #eee;
            --accent: #0f3460;
            --add-bg: #1a4d1a;
            --del-bg: #4d1a1a;
            --add-text: #4ade80;
            --del-text: #f87171;
        }
        @media (prefers-color-scheme: light) {
            :root {
                --bg-color: #f5f5f5;
                --card-bg: #fff;
                --text-color: #333;
                --accent: #e0e0e0;
                --add-bg: #d4edda;
                --del-bg: #f8d7da;
                --add-text: #155724;
                --del-text: #721c24;
            }
        }
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: var(--bg-color);
            color: var(--text-color);
            line-height: 1.6;
            padding: 2rem;
        }
        .container { max-width: 1200px; margin: 0 auto; }
        header { margin-bottom: 2rem; }
        h1 { font-size: 1.5rem; margin-bottom: 0.5rem; }
        .meta { color: #888; font-size: 0.9rem; }
        .summary {
            background: var(--card-bg);
            padding: 1rem;
            border-radius: 8px;
            margin-bottom: 2rem;
            display: flex;
            gap: 2rem;
        }
        .stat { display: flex; align-items: center; gap: 0.5rem; }
        .stat.added { color: var(--add-text); }
        .stat.removed { color: var(--del-text); }
        .analysis {
            background: var(--card-bg);
            padding: 1.5rem;
            border-radius: 8px;
            margin-bottom: 2rem;
            border-left: 4px solid #8b5cf6;
        }
        .analysis-header {
            font-weight: 600;
            margin-bottom: 0.75rem;
            color: #8b5cf6;
        }
        .analysis-content {
            white-space: pre-wrap;
            font-size: 0.95rem;
            line-height: 1.7;
        }
        .diff-container {
            background: var(--card-bg);
            border-radius: 8px;
            overflow: hidden;
        }
        .diff-header {
            background: var(--accent);
            padding: 0.75rem 1rem;
            font-weight: 600;
        }
        .diff-content {
            padding: 1rem;
            overflow-x: auto;
        }
        .diff-content ins {
            background: var(--add-bg);
            color: var(--add-text);
            text-decoration: none;
            padding: 0.1em 0.2em;
            border-radius: 2px;
        }
        .diff-content del {
            background: var(--del-bg);
            color: var(--del-text);
            text-decoration: line-through;
            padding: 0.1em 0.2em;
            border-radius: 2px;
        }
        pre {
            background: var(--accent);
            padding: 1rem;
            border-radius: 8px;
            overflow-x: auto;
            font-size: 0.85rem;
            margin-top: 2rem;
        }
        a { color: #60a5fa; }
        .back-link { margin-bottom: 1rem; display: inline-block; }
    </style>
</head>
<body>
    <div class="container">
        <a href="index.html" class="back-link">&larr; Back to daily report</a>

        <header>
            <h1>build-with-claude/context-editing.md</h1>
            <p class="meta">Changed on 2026-01-06 14:35:17 UTC</p>
        </header>

        <div class="summary">
            <div class="stat added">
                <span>+458</span> lines added
            </div>
            <div class="stat removed">
                <span>-0</span> lines removed
            </div>
        </div>

        
        <div class="analysis">
            <div class="analysis-header">ðŸ¤– AI Analysis</div>
            <div class="analysis-content"># Documentation Change Analysis

## Summary
The documentation for context editing has been significantly expanded, introducing new server-side and client-side strategies for managing conversation context, which can optimize costs and enhance performance.

## Key Changes
- **Introduction of Context Editing**: A new feature that allows automatic management of conversation context to stay within limits and optimize costs.
- **Server-side Strategies**: Detailed explanations of `tool result clearing` and `thinking block clearing`, including how they function and their configuration options.
- **Client-side Compaction**: Introduction of a feature that summarizes conversation history to manage context more effectively, available in Python and TypeScript SDKs.
- **Configuration Options**: Expanded sections on advanced configuration for both server-side and client-side strategies, allowing for more granular control over context management.
- **Integration with Memory Tool**: Instructions on how to combine context editing with the memory tool for preserving important information during long-running workflows.

## Impact
**High**: This change introduces new features and strategies that can significantly improve how developers manage conversation context, potentially leading to better performance and cost efficiency in applications utilizing Claude.</div>
        </div>
        

        <div class="diff-container">
            <div class="diff-header">Visual Diff</div>
            <div class="diff-content"><ins style="background:#e6ffe6;"># Context editing&para;<br>&para;<br>Automatically manage conversation context as it grows with context editing.&para;<br>&para;<br>---&para;<br>&para;<br>## Overview&para;<br>&para;<br>Context editing allows you to automatically manage conversation context as it grows, helping you optimize costs and stay within context window limits. You can use server-side API strategies, client-side SDK features, or both together.&para;<br>&para;<br>| Approach | Where it runs | Strategies | How it works |&para;<br>|----------|---------------|------------|--------------|&para;<br>| **Server-side** | API | Tool result clearing (`clear_tool_uses_20250919`)&lt;br/&gt;Thinking block clearing (`clear_thinking_20251015`) | Applied before the prompt reaches Claude. Clears specific content from conversation history. Each strategy can be configured independently. |&para;<br>| **Client-side** | SDK | Compaction | Available in [Python and TypeScript SDKs](/docs/en/api/client-sdks) when using [`tool_runner`](/docs/en/agents-and-tools/tool-use/implement-tool-use#tool-runner-beta). Generates a summary and replaces full conversation history. See [Compaction](#client-side-compaction-sdk) below. |&para;<br>&para;<br>## Server-side strategies&para;<br>&para;<br>&lt;Note&gt;&para;<br>Context editing is currently in beta with support for tool result clearing and thinking block clearing. To enable it, use the beta header `context-management-2025-06-27` in your API requests.&para;<br>&para;<br>Please reach out through our [feedback form](https://forms.gle/YXC2EKGMhjN1c4L88) to share your feedback on this feature.&para;<br>&lt;/Note&gt;&para;<br>&para;<br>### Tool result clearing&para;<br>&para;<br>The `clear_tool_uses_20250919` strategy clears tool results when conversation context grows beyond your configured threshold. When activated, the API automatically clears the oldest tool results in chronological order, replacing them with placeholder text to let Claude know the tool result was removed. By default, only tool results are cleared. You can optionally clear both tool results and tool calls (the tool use parameters) by setting `clear_tool_inputs` to true.&para;<br>&para;<br>### Thinking block clearing&para;<br>&para;<br>The `clear_thinking_20251015` strategy manages `thinking` blocks in conversations when extended thinking is enabled. This strategy automatically clears older thinking blocks from previous turns.&para;<br>&para;<br>&lt;Tip&gt;&para;<br>**Default behavior**: When extended thinking is enabled without configuring the `clear_thinking_20251015` strategy, the API automatically keeps only the thinking blocks from the last assistant turn (equivalent to `keep: {type: "thinking_turns", value: 1}`).&para;<br>&para;<br>To maximize cache hits, preserve all thinking blocks by setting `keep: "all"`.&para;<br>&lt;/Tip&gt;&para;<br>&para;<br>&lt;Note&gt;&para;<br>An assistant conversation turn may include multiple content blocks (e.g. when using tools) and multiple thinking blocks (e.g. with [interleaved thinking](/docs/en/build-with-claude/extended-thinking#interleaved-thinking)).&para;<br>&lt;/Note&gt;&para;<br>&para;<br>&lt;Tip&gt;&para;<br>**Context editing happens server-side**&para;<br>&para;<br>Context editing is applied **server-side** before the prompt reaches Claude. Your client application maintains the full, unmodified conversation historyâ€”you do not need to sync your client state with the edited version. Continue managing your full conversation history locally as you normally would.&para;<br>&lt;/Tip&gt;&para;<br>&para;<br>&lt;Tip&gt;&para;<br>**Context editing and prompt caching**&para;<br>&para;<br>Context editing's interaction with [prompt caching](/docs/en/build-with-claude/prompt-caching) varies by strategy:&para;<br>&para;<br>- **Tool result clearing**: Invalidates cached prompt prefixes when content is cleared. To account for this, we recommend clearing enough tokens to make the cache invalidation worthwhile. Use the `clear_at_least` parameter to ensure a minimum number of tokens is cleared each time. You'll incur cache write costs each time content is cleared, but subsequent requests can reuse the newly cached prefix.&para;<br>&para;<br>- **Thinking block clearing**: When thinking blocks are **kept** in context (not cleared), the prompt cache is preserved, enabling cache hits and reducing input token costs. When thinking blocks are **cleared**, the cache is invalidated at the point where clearing occurs. Configure the `keep` parameter based on whether you want to prioritize cache performance or context window availability.&para;<br>&lt;/Tip&gt;&para;<br>&para;<br>## Supported models&para;<br>&para;<br>Context editing is available on:&para;<br>&para;<br>- Claude Opus 4.5 (`claude-opus-4-5-20251101`)&para;<br>- Claude Opus 4.1 (`claude-opus-4-1-20250805`)&para;<br>- Claude Opus 4 (`claude-opus-4-20250514`)&para;<br>- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)&para;<br>- Claude Sonnet 4 (`claude-sonnet-4-20250514`)&para;<br>- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)&para;<br>&para;<br>## Tool result clearing usage&para;<br>&para;<br>The simplest way to enable tool result clearing is to specify only the strategy type, as all other [configuration options](#configuration-options-for-tool-result-clearing) will use their default values:&para;<br>&para;<br>&lt;CodeGroup&gt;&para;<br>&para;<br>```bash cURL&para;<br>curl https://api.anthropic.com/v1/messages \&para;<br>    --header "x-api-key: $ANTHROPIC_API_KEY" \&para;<br>    --header "anthropic-version: 2023-06-01" \&para;<br>    --header "content-type: application/json" \&para;<br>    --header "anthropic-beta: context-management-2025-06-27" \&para;<br>    --data '{&para;<br>        "model": "claude-sonnet-4-5",&para;<br>        "max_tokens": 4096,&para;<br>        "messages": [&para;<br>            {&para;<br>                "role": "user",&para;<br>                "content": "Search for recent developments in AI"&para;<br>            }&para;<br>        ],&para;<br>        "tools": [&para;<br>            {&para;<br>                "type": "web_search_20250305",&para;<br>                "name": "web_search"&para;<br>            }&para;<br>        ],&para;<br>        "context_management": {&para;<br>            "edits": [&para;<br>                {"type": "clear_tool_uses_20250919"}&para;<br>            ]&para;<br>        }&para;<br>    }'&para;<br>```&para;<br>&para;<br>```python Python&para;<br>response = client.beta.messages.create(&para;<br>    model="claude-sonnet-4-5",&para;<br>    max_tokens=4096,&para;<br>    messages=[&para;<br>        {&para;<br>            "role": "user",&para;<br>            "content": "Search for recent developments in AI"&para;<br>        }&para;<br>    ],&para;<br>    tools=[&para;<br>        {&para;<br>            "type": "web_search_20250305",&para;<br>            "name": "web_search"&para;<br>        }&para;<br>    ],&para;<br>    betas=["context-management-2025-06-27"],&para;<br>    context_management={&para;<br>        "edits": [&para;<br>            {"type": "clear_tool_uses_20250919"}&para;<br>        ]&para;<br>    }&para;<br>)&para;<br>```&para;<br>&para;<br>```typescript TypeScript&para;<br>import Anthropic from '@anthropic-ai/sdk';&para;<br>&para;<br>const anthropic = new Anthropic({&para;<br>  apiKey: process.env.ANTHROPIC_API_KEY,&para;<br>});&para;<br>&para;<br>const response = await anthropic.beta.messages.create({&para;<br>  model: "claude-sonnet-4-5",&para;<br>  max_tokens: 4096,&para;<br>  messages: [&para;<br>    {&para;<br>      role: "user",&para;<br>      content: "Search for recent developments in AI"&para;<br>    }&para;<br>  ],&para;<br>  tools: [&para;<br>    {&para;<br>      type: "web_search_20250305",&para;<br>      name: "web_search"&para;<br>    }&para;<br>  ],&para;<br>  context_management: {&para;<br>    edits: [&para;<br>      { type: "clear_tool_uses_20250919" }&para;<br>    ]&para;<br>  },&para;<br>  betas: ["context-management-2025-06-27"]&para;<br>});&para;<br>```&para;<br>&para;<br>&lt;/CodeGroup&gt;&para;<br>&para;<br>### Advanced configuration&para;<br>&para;<br>You can customize the tool result clearing behavior with additional parameters:&para;<br>&para;<br>&lt;CodeGroup&gt;&para;<br>&para;<br>```bash cURL&para;<br>curl https://api.anthropic.com/v1/messages \&para;<br>    --header "x-api-key: $ANTHROPIC_API_KEY" \&para;<br>    --header "anthropic-version: 2023-06-01" \&para;<br>    --header "content-type: application/json" \&para;<br>    --header "anthropic-beta: context-management-2025-06-27" \&para;<br>    --data '{&para;<br>        "model": "claude-sonnet-4-5",&para;<br>        "max_tokens": 4096,&para;<br>        "messages": [&para;<br>            {&para;<br>                "role": "user",&para;<br>                "content": "Create a simple command line calculator app using Python"&para;<br>            }&para;<br>        ],&para;<br>        "tools": [&para;<br>            {&para;<br>                "type": "text_editor_20250728",&para;<br>                "name": "str_replace_based_edit_tool",&para;<br>                "max_characters": 10000&para;<br>            },&para;<br>            {&para;<br>                "type": "web_search_20250305",&para;<br>                "name": "web_search",&para;<br>                "max_uses": 3&para;<br>            }&para;<br>        ],&para;<br>        "context_management": {&para;<br>            "edits": [&para;<br>                {&para;<br>                    "type": "clear_tool_uses_20250919",&para;<br>                    "trigger": {&para;<br>                        "type": "input_tokens",&para;<br>                        "value": 30000&para;<br>                    },&para;<br>                    "keep": {&para;<br>                        "type": "tool_uses",&para;<br>                        "value": 3&para;<br>                    },&para;<br>                    "clear_at_least": {&para;<br>                        "type": "input_tokens",&para;<br>                        "value": 5000&para;<br>                    },&para;<br>                    "exclude_tools": ["web_search"]&para;<br>                }&para;<br>            ]&para;<br>        }&para;<br>    }'&para;<br>```&para;<br>&para;<br>```python Python&para;<br>response = client.beta.messages.create(&para;<br>    model="claude-sonnet-4-5",&para;<br>    max_tokens=4096,&para;<br>    messages=[&para;<br>        {&para;<br>            "role": "user",&para;<br>            "content": "Create a simple command line calculator app using Python"&para;<br>        }&para;<br>    ],&para;<br>    tools=[&para;<br>        {&para;<br>            "type": "text_editor_20250728",&para;<br>            "name": "str_replace_based_edit_tool",&para;<br>            "max_characters": 10000&para;<br>        },&para;<br>        {&para;<br>            "type": "web_search_20250305",&para;<br>            "name": "web_search",&para;<br>            "max_uses": 3&para;<br>        }&para;<br>    ],&para;<br>    betas=["context-management-2025-06-27"],&para;<br>    context_management={&para;<br>        "edits": [&para;<br>            {&para;<br>                "type": "clear_tool_uses_20250919",&para;<br>                # Trigger clearing when threshold is exceeded&para;<br>                "trigger": {&para;<br>                    "type": "input_tokens",&para;<br>                    "value": 30000&para;<br>                },&para;<br>                # Number of tool uses to keep after clearing&para;<br>                "keep": {&para;<br>                    "type": "tool_uses",&para;<br>                    "value": 3&para;<br>                },&para;<br>                # Optional: Clear at least this many tokens&para;<br>                "clear_at_least": {&para;<br>                    "type": "input_tokens",&para;<br>                    "value": 5000&para;<br>                },&para;<br>                # Exclude these tools from being cleared&para;<br>                "exclude_tools": ["web_search"]&para;<br>            }&para;<br>        ]&para;<br>    }&para;<br>)&para;<br>```&para;<br>&para;<br>```typescript TypeScript&para;<br>import Anthropic from '@anthropic-ai/sdk';&para;<br>&para;<br>const anthropic = new Anthropic({&para;<br>  apiKey: process.env.ANTHROPIC_API_KEY,&para;<br>});&para;<br>&para;<br>const response = await anthropic.beta.messages.create({&para;<br>  model: "claude-sonnet-4-5",&para;<br>  max_tokens: 4096,&para;<br>  messages: [&para;<br>    {&para;<br>      role: "user",&para;<br>      content: "Create a simple command line calculator app using Python"&para;<br>    }&para;<br>  ],&para;<br>  tools: [&para;<br>    {&para;<br>      type: "text_editor_20250728",&para;<br>      name: "str_replace_based_edit_tool",&para;<br>      max_characters: 10000&para;<br>    },&para;<br>    {&para;<br>      type: "web_search_20250305",&para;<br>      name: "web_search",&para;<br>      max_uses: 3&para;<br>    }&para;<br>  ],&para;<br>  betas: ["context-management-2025-06-27"],&para;<br>  context_management: {&para;<br>    edits: [&para;<br>      {&para;<br>        type: "clear_tool_uses_20250919",&para;<br>        // Trigger clearing when threshold is exceeded&para;<br>        trigger: {&para;<br>          type: "input_tokens",&para;<br>          value: 30000&para;<br>        },&para;<br>        // Number of tool uses to keep after clearing&para;<br>        keep: {&para;<br>          type: "tool_uses",&para;<br>          value: 3&para;<br>        },&para;<br>        // Optional: Clear at least this many tokens&para;<br>        clear_at_least: {&para;<br>          type: "input_tokens",&para;<br>          value: 5000&para;<br>        },&para;<br>        // Exclude these tools from being cleared&para;<br>        exclude_tools: ["web_search"]&para;<br>      }&para;<br>    ]&para;<br>  }&para;<br>});&para;<br>```&para;<br>&para;<br>&lt;/CodeGroup&gt;&para;<br>&para;<br>## Thinking block clearing usage&para;<br>&para;<br>Enable thinking block clearing to manage context and prompt caching effectively when extended thinking is enabled:&para;<br>&para;<br>&lt;CodeGroup&gt;&para;<br>&para;<br>```bash cURL&para;<br>curl https://api.anthropic.com/v1/messages \&para;<br>    --header "x-api-key: $ANTHROPIC_API_KEY" \&para;<br>    --header "anthropic-version: 2023-06-01" \&para;<br>    --header "content-type: application/json" \&para;<br>    --header "anthropic-beta: context-management-2025-06-27" \&para;<br>    --data '{&para;<br>        "model": "claude-sonnet-4-5-20250929",&para;<br>        "max_tokens": 1024,&para;<br>        "messages": [...],&para;<br>        "thinking": {&para;<br>            "type": "enabled",&para;<br>            "budget_tokens": 10000&para;<br>        },&para;<br>        "context_management": {&para;<br>            "edits": [&para;<br>                {&para;<br>                    "type": "clear_thinking_20251015",&para;<br>                    "keep": {&para;<br>                        "type": "thinking_turns",&para;<br>                        "value": 2&para;<br>                    }&para;<br>                }&para;<br>            ]&para;<br>        }&para;<br>    }'&para;<br>```&para;<br>&para;<br>```python Python&para;<br>response = client.beta.messages.create(&para;<br>    model="claude-sonnet-4-5-20250929",&para;<br>    max_tokens=1024,&para;<br>    messages=[...],&para;<br>    thinking={&para;<br>        "type": "enabled",&para;<br>        "budget_tokens": 10000&para;<br>    },&para;<br>    betas=["context-management-2025-06-27"],&para;<br>    context_management={&para;<br>        "edits": [&para;<br>            {&para;<br>                "type": "clear_thinking_20251015",&para;<br>                "keep": {&para;<br>                    "type": "thinking_turns",&para;<br>                    "value": 2&para;<br>                }&para;<br>            }&para;<br>        ]&para;<br>    }&para;<br>)&para;<br>```&para;<br>&para;<br>```typescript TypeScript&para;<br>import Anthropic from '@anthropic-ai/sdk';&para;<br>&para;<br>const anthropic = new Anthropic({&para;<br>  apiKey: process.env.ANTHROPIC_API_KEY,&para;<br>});&para;<br>&para;<br>const response = await anthropic.beta.messages.create({&para;<br>  model: "claude-sonnet-4-5-20250929",&para;<br>  max_tokens: 1024,&para;<br>  messages: [...],&para;<br>  thinking: {&para;<br>    type: "enabled",&para;<br>    budget_tokens: 10000&para;<br>  },&para;<br>  betas: ["context-management-2025-06-27"],&para;<br>  context_management: {&para;<br>    edits: [&para;<br>      {&para;<br>        type: "clear_thinking_20251015",&para;<br>        keep: {&para;<br>          type: "thinking_turns",&para;<br>          value: 2&para;<br>        }&para;<br>      }&para;<br>    ]&para;<br>  }&para;<br>});&para;<br>```&para;<br>&para;<br>&lt;/CodeGroup&gt;&para;<br>&para;<br>### Configuration options for thinking block clearing&para;<br>&para;<br>The `clear_thinking_20251015` strategy supports the following configuration:&para;<br>&para;<br>| Configuration option | Default | Description |&para;<br>|---------------------|---------|-------------|&para;<br>| `keep` | `{type: "thinking_turns", value: 1}` | Defines how many recent assistant turns with thinking blocks to preserve. Use `{type: "thinking_turns", value: N}` where N must be &gt; 0 to keep the last N turns, or `"all"` to keep all thinking blocks. |&para;<br>&para;<br>**Example configurations:**&para;<br>&para;<br>```json&para;<br>// Keep thinking blocks from the last 3 assistant turns&para;<br>{&para;<br>  "type": "clear_thinking_20251015",&para;<br>  "keep": {&para;<br>    "type": "thinking_turns",&para;<br>    "value": 3&para;<br>  }&para;<br>}&para;<br>&para;<br>// Keep all thinking blocks (maximizes cache hits)&para;<br>{&para;<br>  "type": "clear_thinking_20251015",&para;<br>  "keep": "all"&para;<br>}&para;<br>```&para;<br>&para;<br>### Combining strategies&para;<br>&para;<br>You can use both thinking block clearing and tool result clearing together:&para;<br>&para;<br>&lt;Note&gt;&para;<br>When using multiple strategies, the `clear_thinking_20251015` strategy must be listed first in the `edits` array.&para;<br>&lt;/Note&gt;&para;<br>&para;<br>&lt;CodeGroup&gt;&para;<br>&para;<br>```python Python&para;<br>response = client.beta.messages.create(&para;<br>    model="claude-sonnet-4-5-20250929",&para;<br>    max_tokens=1024,&para;<br>    messages=[...],&para;<br>    thinking={&para;<br>        "type": "enabled",&para;<br>        "budget_tokens": 10000&para;<br>    },&para;<br>    tools=[...],&para;<br>    betas=["context-management-2025-06-27"],&para;<br>    context_management={&para;<br>        "edits": [&para;<br>            {&para;<br>                "type": "clear_thinking_20251015",&para;<br>                "keep": {&para;<br>                    "type": "thinking_turns",&para;<br>                    "value": 2&para;<br>                }&para;<br>            },&para;<br>            {&para;<br>                "type": "clear_tool_uses_20250919",&para;<br>                "trigger": {&para;<br>                    "type": "input_tokens",&para;<br>                    "value": 50000&para;<br>                },&para;<br>                "keep": {&para;<br>                    "type": "tool_uses",&para;<br>                    "value": 5&para;<br>                }&para;<br>            }&para;<br>        ]&para;<br>    }&para;<br>)&para;<br>```&para;<br>&para;<br>```typescript TypeScript&para;<br>const response = await anthropic.beta.messages.create({&para;<br>  model: "claude-sonnet-4-5-20250929",&para;<br>  max_tokens: 1024,&para;<br>  messages: [...],&para;<br>  thinking: {&para;<br>    type: "enabled",&para;<br>    budget_tokens: 10000&para;<br>  },&para;<br>  tools: [...],&para;<br>  betas: ["context-management-2025-06-27"],&para;<br>  context_management: {&para;<br>    edits: [&para;<br>      {&para;<br>        type: "clear_thinking_20251015",&para;<br>        keep: {&para;<br>          type: "thinking_turns",&para;<br>          value: 2&para;<br>        }&para;<br>      },&para;<br>      {&para;<br>        type: "clear_tool_uses_20250919",&para;<br>        trigger: {&para;<br>          type: "input_tokens",&para;<br>          value: 50000&para;<br>        },&para;<br>        keep: {&para;<br>          type: "tool_uses",&para;<br>          value: 5&para;<br>        }&para;<br>      }&para;<br>    ]&para;<br>  }&para;<br>});&para;<br>```&para;<br>&para;<br>&lt;/CodeGroup&gt;&para;<br>&para;<br>## Configuration options for tool result clearing&para;<br>&para;<br>| Configuration option | Default | Description |&para;<br>|---------------------|---------|-------------|&para;<br>| `trigger` | 100,000 input tokens | Defines when the context editing strategy activates. Once the prompt exceeds this threshold, clearing will begin. You can specify this value in either `input_tokens` or `tool_uses`. |&para;<br>| `keep` | 3 tool uses | Defines how many recent tool use/result pairs to keep after clearing occurs. The API removes the oldest tool interactions first, preserving the most recent ones. |&para;<br>| `clear_at_least` | None | Ensures a minimum number of tokens is cleared each time the strategy activates. If the API can't clear at least the specified amount, the strategy will not be applied. This helps determine if context clearing is worth breaking your prompt cache. |&para;<br>| `exclude_tools` | None | List of tool names whose tool uses and results should never be cleared. Useful for preserving important context. |&para;<br>| `clear_tool_inputs` | `false` | Controls whether the tool call parameters are cleared along with the tool results. By default, only the tool results are cleared while keeping Claude's original tool calls visible. |&para;<br>&para;<br>## Context editing response&para;<br>&para;<br>You can see which context edits were applied to your request using the `context_management` response field, along with helpful statistics about the content and input tokens cleared.&para;<br>&para;<br>```json Response&para;<br>{&para;<br>    "id": "msg_013Zva2CMHLNnXjNJJKqJ2EF",&para;<br>    "type": "message",&para;<br>    "role": "assistant",&para;<br>    "content": [...],&para;<br>    "usage": {...},&para;<br>    "context_management": {&para;<br>        "applied_edits": [&para;<br>            // When using `clear_thinking_20251015`&para;<br>            {&para;<br>                "type": "clear_thinking_20251015",&para;<br>                "cleared_thinking_turns": 3,&para;<br>                "cleared_input_tokens": 15000&para;<br>            },&para;<br>            // When using `clear_tool_uses_20250919`&para;<br>            {&para;<br>                "type": "clear_tool_uses_20250919",&para;<br>                "cleared_tool_uses": 8,&para;<br>                "cleared_input_tokens": 50000&para;<br>            }&para;<br>        ]&para;<br>    }&para;<br>}&para;<br>```&para;<br>&para;<br>For streaming responses, the context edits will be included in the final `message_delta` event:&para;<br>&para;<br>```json Streaming Response&para;<br>{&para;<br>    "type": "message_delta",&para;<br>    "delta": {&para;<br>        "stop_reason": "end_turn",&para;<br>        "stop_sequence": null&para;<br>    },&para;<br>    "usage": {&para;<br>        "output_tokens": 1024&para;<br>    },&para;<br>    "context_management": {&para;<br>        "applied_edits": [...]&para;<br>    }&para;<br>}&para;<br>```&para;<br>&para;<br>## Token counting&para;<br>&para;<br>The [token counting](/docs/en/build-with-claude/token-counting) endpoint supports context management, allowing you to preview how many tokens your prompt will use after context editing is applied.&para;<br>&para;<br>&lt;CodeGroup&gt;&para;<br>&para;<br>```bash cURL&para;<br>curl https://api.anthropic.com/v1/messages/count_tokens \&para;<br>    --header "x-api-key: $ANTHROPIC_API_KEY" \&para;<br>    --header "anthropic-version: 2023-06-01" \&para;<br>    --header "content-type: application/json" \&para;<br>    --header "anthropic-beta: context-management-2025-06-27" \&para;<br>    --data '{&para;<br>        "model": "claude-sonnet-4-5",&para;<br>        "messages": [&para;<br>            {&para;<br>                "role": "user",&para;<br>                "content": "Continue our conversation..."&para;<br>            }&para;<br>        ],&para;<br>        "tools": [...],&para;<br>        "context_management": {&para;<br>            "edits": [&para;<br>                {&para;<br>                    "type": "clear_tool_uses_20250919",&para;<br>                    "trigger": {&para;<br>                        "type": "input_tokens",&para;<br>                        "value": 30000&para;<br>                    },&para;<br>                    "keep": {&para;<br>                        "type": "tool_uses",&para;<br>                        "value": 5&para;<br>                    }&para;<br>                }&para;<br>            ]&para;<br>        }&para;<br>    }'&para;<br>```&para;<br>&para;<br>```python Python&para;<br>response = client.beta.messages.count_tokens(&para;<br>    model="claude-sonnet-4-5",&para;<br>    messages=[&para;<br>        {&para;<br>            "role": "user",&para;<br>            "content": "Continue our conversation..."&para;<br>        }&para;<br>    ],&para;<br>    tools=[...],  # Your tool definitions&para;<br>    betas=["context-management-2025-06-27"],&para;<br>    context_management={&para;<br>        "edits": [&para;<br>            {&para;<br>                "type": "clear_tool_uses_20250919",&para;<br>                "trigger": {&para;<br>                    "type": "input_tokens",&para;<br>                    "value": 30000&para;<br>                },&para;<br>                "keep": {&para;<br>                    "type": "tool_uses",&para;<br>                    "value": 5&para;<br>                }&para;<br>            }&para;<br>        ]&para;<br>    }&para;<br>)&para;<br>&para;<br>print(f"Original tokens: {response.context_management['original_input_tokens']}")&para;<br>print(f"After clearing: {response.input_tokens}")&para;<br>print(f"Savings: {response.context_management['original_input_tokens'] - response.input_tokens} tokens")&para;<br>```&para;<br>&para;<br>```typescript TypeScript&para;<br>import Anthropic from '@anthropic-ai/sdk';&para;<br>&para;<br>const anthropic = new Anthropic({&para;<br>  apiKey: process.env.ANTHROPIC_API_KEY,&para;<br>});&para;<br>&para;<br>const response = await anthropic.beta.messages.countTokens({&para;<br>  model: "claude-sonnet-4-5",&para;<br>  messages: [&para;<br>    {&para;<br>      role: "user",&para;<br>      content: "Continue our conversation..."&para;<br>    }&para;<br>  ],&para;<br>  tools: [...],  // Your tool definitions&para;<br>  betas: ["context-management-2025-06-27"],&para;<br>  context_management: {&para;<br>    edits: [&para;<br>      {&para;<br>        type: "clear_tool_uses_20250919",&para;<br>        trigger: {&para;<br>          type: "input_tokens",&para;<br>          value: 30000&para;<br>        },&para;<br>        keep: {&para;<br>          type: "tool_uses",&para;<br>          value: 5&para;<br>        }&para;<br>      }&para;<br>    ]&para;<br>  }&para;<br>});&para;<br>&para;<br>console.log(`Original tokens: ${response.context_management?.original_input_tokens}`);&para;<br>console.log(`After clearing: ${response.input_tokens}`);&para;<br>console.log(`Savings: ${(response.context_management?.original_input_tokens || 0) - response.input_tokens} tokens`);&para;<br>```&para;<br>&lt;/CodeGroup&gt;&para;<br>&para;<br>```json Response&para;<br>{&para;<br>    "input_tokens": 25000,&para;<br>    "context_management": {&para;<br>        "original_input_tokens": 70000&para;<br>    }&para;<br>}&para;<br>```&para;<br>&para;<br>The response shows both the final token count after context management is applied (`input_tokens`) and the original token count before any clearing occurred (`original_input_tokens`).&para;<br>&para;<br>## Using with the Memory Tool&para;<br>&para;<br>Context editing can be combined with the [memory tool](/docs/en/agents-and-tools/tool-use/memory-tool). When your conversation context approaches the configured clearing threshold, Claude receives an automatic warning to preserve important information. This enables Claude to save tool results or context to its memory files before they're cleared from the conversation history.&para;<br>&para;<br>This combination allows you to:&para;<br>&para;<br>- **Preserve important context**: Claude can write essential information from tool results to memory files before those results are cleared&para;<br>- **Maintain long-running workflows**: Enable agentic workflows that would otherwise exceed context limits by offloading information to persistent storage&para;<br>- **Access information on demand**: Claude can look up previously cleared information from memory files when needed, rather than keeping everything in the active context window&para;<br>&para;<br>For example, in a file editing workflow where Claude performs many operations, Claude can summarize completed changes to memory files as the context grows. When tool results are cleared, Claude retains access to that information through its memory system and can continue working effectively.&para;<br>&para;<br>To use both features together, enable them in your API request:&para;<br>&para;<br>&lt;CodeGroup&gt;&para;<br>&para;<br>```python Python&para;<br>response = client.beta.messages.create(&para;<br>    model="claude-sonnet-4-5",&para;<br>    max_tokens=4096,&para;<br>    messages=[...],&para;<br>    tools=[&para;<br>        {&para;<br>            "type": "memory_20250818",&para;<br>            "name": "memory"&para;<br>        },&para;<br>        # Your other tools&para;<br>    ],&para;<br>    betas=["context-management-2025-06-27"],&para;<br>    context_management={&para;<br>        "edits": [&para;<br>            {"type": "clear_tool_uses_20250919"}&para;<br>        ]&para;<br>    }&para;<br>)&para;<br>```&para;<br>&para;<br>```typescript TypeScript&para;<br>import Anthropic from '@anthropic-ai/sdk';&para;<br>&para;<br>const anthropic = new Anthropic({&para;<br>  apiKey: process.env.ANTHROPIC_API_KEY,&para;<br>});&para;<br>&para;<br>const response = await anthropic.beta.messages.create({&para;<br>  model: "claude-sonnet-4-5",&para;<br>  max_tokens: 4096,&para;<br>  messages: [...],&para;<br>  tools: [&para;<br>    {&para;<br>      type: "memory_20250818",&para;<br>      name: "memory"&para;<br>    },&para;<br>    // Your other tools&para;<br>  ],&para;<br>  betas: ["context-management-2025-06-27"],&para;<br>  context_management: {&para;<br>    edits: [&para;<br>      { type: "clear_tool_uses_20250919" }&para;<br>    ]&para;<br>  }&para;<br>});&para;<br>```&para;<br>&para;<br>&lt;/CodeGroup&gt;&para;<br>&para;<br>## Client-side compaction (SDK)&para;<br>&para;<br>&lt;Note&gt;&para;<br>Compaction is available in the [Python and TypeScript SDKs](/docs/en/api/client-sdks) when using the [`tool_runner` method](/docs/en/agents-and-tools/tool-use/implement-tool-use#tool-runner-beta).&para;<br>&lt;/Note&gt;&para;<br>&para;<br>Compaction is an SDK feature that automatically manages conversation context by generating summaries when token usage grows too large. Unlike server-side context editing strategies that clear content, compaction instructs Claude to summarize the conversation history, then replaces the full history with that summary. This allows Claude to continue working on long-running tasks that would otherwise exceed the [context window](/docs/en/build-with-claude/context-windows).&para;<br>&para;<br>### How compaction works&para;<br>&para;<br>When compaction is enabled, the SDK monitors token usage after each model response:&para;<br>&para;<br>1. **Threshold check**: The SDK calculates total tokens as `input_tokens + cache_creation_input_tokens + cache_read_input_tokens + output_tokens`&para;<br>2. **Summary generation**: When the threshold is exceeded, a summary prompt is injected as a user turn, and Claude generates a structured summary wrapped in `&lt;summary&gt;&lt;/summary&gt;` tags&para;<br>3. **Context replacement**: The SDK extracts the summary and replaces the entire message history with it&para;<br>4. **Continuation**: The conversation resumes from the summary, with Claude picking up where it left off&para;<br>&para;<br>### Using compaction&para;<br>&para;<br>Add `compaction_control` to your `tool_runner` call:&para;<br>&para;<br>&lt;CodeGroup&gt;&para;<br>&para;<br>```python Python&para;<br>import anthropic&para;<br>&para;<br>client = anthropic.Anthropic()&para;<br>&para;<br>runner = client.beta.messages.tool_runner(&para;<br>    model="claude-sonnet-4-5",&para;<br>    max_tokens=4096,&para;<br>    tools=[...],&para;<br>    messages=[&para;<br>        {&para;<br>            "role": "user",&para;<br>            "content": "Analyze all the files in this directory and write a summary report."&para;<br>        }&para;<br>    ],&para;<br>    compaction_control={&para;<br>        "enabled": True,&para;<br>        "context_token_threshold": 100000&para;<br>    }&para;<br>)&para;<br>&para;<br>for message in runner:&para;<br>    print(f"Tokens used: {message.usage.input_tokens}")&para;<br>&para;<br>final = runner.until_done()&para;<br>```&para;<br>&para;<br>```typescript TypeScript&para;<br>import Anthropic from '@anthropic-ai/sdk';&para;<br>&para;<br>const client = new Anthropic();&para;<br>&para;<br>const runner = client.beta.messages.toolRunner({&para;<br>    model: 'claude-sonnet-4-5',&para;<br>    max_tokens: 4096,&para;<br>    tools: [...],&para;<br>    messages: [&para;<br>        {&para;<br>            role: 'user',&para;<br>            content: 'Analyze all the files in this directory and write a summary report.'&para;<br>        }&para;<br>    ],&para;<br>    compactionControl: {&para;<br>        enabled: true,&para;<br>        contextTokenThreshold: 100000&para;<br>    }&para;<br>});&para;<br>&para;<br>for await (const message of runner) {&para;<br>    console.log('Tokens used:', message.usage.input_tokens);&para;<br>}&para;<br>&para;<br>const finalMessage = await runner.runUntilDone();&para;<br>```&para;<br>&para;<br>&lt;/CodeGroup&gt;&para;<br>&para;<br>#### What happens during compaction&para;<br>&para;<br>As the conversation grows, the message history accumulates:&para;<br>&para;<br>**Before compaction (approaching 100k tokens):**&para;<br>```json&para;<br>[&para;<br>  { "role": "user", "content": "Analyze all files and write a report..." },&para;<br>  { "role": "assistant", "content": "I'll help. Let me start by reading..." },&para;<br>  { "role": "user", "content": [{ "type": "tool_result", "tool_use_id": "...", "content": "..." }] },&para;<br>  { "role": "assistant", "content": "Based on file1.txt, I see..." },&para;<br>  { "role": "user", "content": [{ "type": "tool_result", "tool_use_id": "...", "content": "..." }] },&para;<br>  { "role": "assistant", "content": "After analyzing file2.txt..." },&para;<br>  // ... 50 more exchanges like this ...&para;<br>]&para;<br>```&para;<br>&para;<br>When tokens exceed the threshold, the SDK injects a summary request and Claude generates a summary. The entire history is then replaced:&para;<br>&para;<br>**After compaction (back to ~2-3k tokens):**&para;<br>```json&para;<br>[&para;<br>  {&para;<br>    "role": "assistant",&para;<br>    "content": "# Task Overview\nThe user requested analysis of directory files to produce a summary report...\n\n# Current State\nAnalyzed 52 files across 3 subdirectories. Key findings documented in report.md...\n\n# Important Discoveries\n- Configuration files use YAML format\n- Found 3 deprecated dependencies\n- Test coverage at 67%\n\n# Next Steps\n1. Analyze remaining files in /src/legacy\n2. Complete final report sections...\n\n# Context to Preserve\nUser prefers markdown format with executive summary first..."&para;<br>  }&para;<br>]&para;<br>```&para;<br>&para;<br>Claude continues working from this summary as if it were the original conversation history.&para;<br>&para;<br>### Configuration options&para;<br>&para;<br>| Parameter | Type | Required | Default | Description |&para;<br>|-----------|------|----------|---------|-------------|&para;<br>| `enabled` | boolean | Yes | - | Whether to enable automatic compaction |&para;<br>| `context_token_threshold` | number | No | 100,000 | Token count at which compaction triggers |&para;<br>| `model` | string | No | Same as main model | Model to use for generating summaries |&para;<br>| `summary_prompt` | string | No | See below | Custom prompt for summary generation |&para;<br>&para;<br>#### Choosing a token threshold&para;<br>&para;<br>The threshold determines when compaction occurs. A lower threshold means more frequent compactions with smaller context windows. A higher threshold allows more context but risks hitting limits.&para;<br>&para;<br>&lt;CodeGroup&gt;&para;<br>&para;<br>```python Python&para;<br># More frequent compaction for memory-constrained scenarios&para;<br>compaction_control={&para;<br>    "enabled": True,&para;<br>    "context_token_threshold": 50000&para;<br>}&para;<br>&para;<br># Less frequent compaction when you need more context&para;<br>compaction_control={&para;<br>    "enabled": True,&para;<br>    "context_token_threshold": 150000&para;<br>}&para;<br>```&para;<br>&para;<br>```typescript TypeScript&para;<br>// More frequent compaction for memory-constrained scenarios&para;<br>compactionControl: {&para;<br>    enabled: true,&para;<br>    contextTokenThreshold: 50000&para;<br>}&para;<br>&para;<br>// Less frequent compaction when you need more context&para;<br>compactionControl: {&para;<br>    enabled: true,&para;<br>    contextTokenThreshold: 150000&para;<br>}&para;<br>```&para;<br>&para;<br>&lt;/CodeGroup&gt;&para;<br>&para;<br>#### Using a different model for summaries&para;<br>&para;<br>You can use a faster or cheaper model for generating summaries:&para;<br>&para;<br>&lt;CodeGroup&gt;&para;<br>&para;<br>```python Python&para;<br>compaction_control={&para;<br>    "enabled": True,&para;<br>    "context_token_threshold": 100000,&para;<br>    "model": "claude-haiku-4-5"&para;<br>}&para;<br>```&para;<br>&para;<br>```typescript TypeScript&para;<br>compactionControl: {&para;<br>    enabled: true,&para;<br>    contextTokenThreshold: 100000,&para;<br>    model: 'claude-haiku-4-5'&para;<br>}&para;<br>```&para;<br>&para;<br>&lt;/CodeGroup&gt;&para;<br>&para;<br>#### Custom summary prompts&para;<br>&para;<br>You can provide a custom prompt for domain-specific needs. Your prompt should instruct Claude to wrap its summary in `&lt;summary&gt;&lt;/summary&gt;` tags.&para;<br>&para;<br>&lt;CodeGroup&gt;&para;<br>&para;<br>```python Python&para;<br>compaction_control={&para;<br>    "enabled": True,&para;<br>    "context_token_threshold": 100000,&para;<br>    "summary_prompt": """Summarize the research conducted so far, including:&para;<br>- Sources consulted and key findings&para;<br>- Questions answered and remaining unknowns&para;<br>- Recommended next steps&para;<br>&para;<br>Wrap your summary in &lt;summary&gt;&lt;/summary&gt; tags."""&para;<br>}&para;<br>```&para;<br>&para;<br>```typescript TypeScript&para;<br>compactionControl: {&para;<br>    enabled: true,&para;<br>    contextTokenThreshold: 100000,&para;<br>    summaryPrompt: `Summarize the research conducted so far, including:&para;<br>- Sources consulted and key findings&para;<br>- Questions answered and remaining unknowns&para;<br>- Recommended next steps&para;<br>&para;<br>Wrap your summary in &lt;summary&gt;&lt;/summary&gt; tags.`&para;<br>}&para;<br>```&para;<br>&para;<br>&lt;/CodeGroup&gt;&para;<br>&para;<br>### Default summary prompt&para;<br>&para;<br>The built-in summary prompt instructs Claude to create a structured continuation summary including:&para;<br>&para;<br>1. **Task Overview**: The user's core request, success criteria, and constraints&para;<br>2. **Current State**: What has been completed, files modified, and artifacts produced&para;<br>3. **Important Discoveries**: Technical constraints, decisions made, errors resolved, and failed approaches&para;<br>4. **Next Steps**: Specific actions needed, blockers, and priority order&para;<br>5. **Context to Preserve**: User preferences, domain-specific details, and commitments made&para;<br>&para;<br>This structure enables Claude to resume work efficiently without losing important context or repeating mistakes.&para;<br>&para;<br>&lt;section title="View full default prompt"&gt;&para;<br>&para;<br>```&para;<br>You have been working on the task described above but have not yet completed it. Write a continuation summary that will allow you (or another instance of yourself) to resume work efficiently in a future context window where the conversation history will be replaced with this summary. Your summary should be structured, concise, and actionable. Include:&para;<br>&para;<br>1. Task Overview&para;<br>The user's core request and success criteria&para;<br>Any clarifications or constraints they specified&para;<br>&para;<br>2. Current State&para;<br>What has been completed so far&para;<br>Files created, modified, or analyzed (with paths if relevant)&para;<br>Key outputs or artifacts produced&para;<br>&para;<br>3. Important Discoveries&para;<br>Technical constraints or requirements uncovered&para;<br>Decisions made and their rationale&para;<br>Errors encountered and how they were resolved&para;<br>What approaches were tried that didn't work (and why)&para;<br>&para;<br>4. Next Steps&para;<br>Specific actions needed to complete the task&para;<br>Any blockers or open questions to resolve&para;<br>Priority order if multiple steps remain&para;<br>&para;<br>5. Context to Preserve&para;<br>User preferences or style requirements&para;<br>Domain-specific details that aren't obvious&para;<br>Any promises made to the user&para;<br>&para;<br>Be concise but completeâ€”err on the side of including information that would prevent duplicate work or repeated mistakes. Write in a way that enables immediate resumption of the task.&para;<br>&para;<br>Wrap your summary in &lt;summary&gt;&lt;/summary&gt; tags.&para;<br>```&para;<br>&para;<br>&lt;/section&gt;&para;<br>&para;<br>### Limitations&para;<br>&para;<br>#### Server-side tools&para;<br>&para;<br>&lt;Warning&gt;&para;<br>Compaction requires special consideration when using server-side tools such as [web search](/docs/en/agents-and-tools/tool-use/web-search-tool) or [web fetch](/docs/en/agents-and-tools/tool-use/web-fetch-tool).&para;<br>&lt;/Warning&gt;&para;<br>&para;<br>When using server-side tools, the SDK may incorrectly calculate token usage, causing compaction to trigger at the wrong time.&para;<br>&para;<br>For example, after a web search operation, the API response might show:&para;<br>&para;<br>```json&para;<br>{&para;<br>  "usage": {&para;<br>    "input_tokens": 63000,&para;<br>    "cache_read_input_tokens": 270000,&para;<br>    "output_tokens": 1400&para;<br>  }&para;<br>}&para;<br>```&para;<br>&para;<br>The SDK calculates total usage as 63,000 + 270,000 = 333,000 tokens. However, the `cache_read_input_tokens` value includes accumulated reads from multiple internal API calls made by the server-side tool, not your actual conversation context. Your real context length might only be the 63,000 `input_tokens`, but the SDK sees 333k and triggers compaction prematurely.&para;<br>&para;<br>**Workarounds:**&para;<br>&para;<br>- Use the [token counting](/docs/en/build-with-claude/token-counting) endpoint to get accurate context length&para;<br>- Avoid compaction when using server-side tools extensively&para;<br>&para;<br>#### Tool use edge cases&para;<br>&para;<br>When compaction is triggered while a tool use response is pending, the SDK removes the tool use block from the message history before generating the summary. Claude will re-issue the tool call after resuming from the summary if still needed.&para;<br>&para;<br>### Monitoring compaction&para;<br>&para;<br>Enable logging to track when compaction occurs:&para;<br>&para;<br>&lt;CodeGroup&gt;&para;<br>&para;<br>```python Python&para;<br>import logging&para;<br>&para;<br>logging.basicConfig(level=logging.INFO)&para;<br>logging.getLogger("anthropic.lib.tools").setLevel(logging.INFO)&para;<br>&para;<br># Logs will show:&para;<br># INFO: Token usage 105000 has exceeded the threshold of 100000. Performing compaction.&para;<br># INFO: Compaction complete. New token usage: 2500&para;<br>```&para;<br>&para;<br>```typescript TypeScript&para;<br>// The SDK logs compaction events to the console&para;<br>// You'll see messages like:&para;<br>// Token usage 105000 has exceeded the threshold of 100000. Performing compaction.&para;<br>// Compaction complete. New token usage: 2500&para;<br>```&para;<br>&para;<br>&lt;/CodeGroup&gt;&para;<br>&para;<br>### When to use compaction&para;<br>&para;<br>**Good use cases:**&para;<br>&para;<br>- Long-running agent tasks that process many files or data sources&para;<br>- Research workflows that accumulate large amounts of information&para;<br>- Multi-step tasks with clear, measurable progress&para;<br>- Tasks that produce artifacts (files, reports) that persist outside the conversation&para;<br>&para;<br>**Less ideal use cases:**&para;<br>&para;<br>- Tasks requiring precise recall of early conversation details&para;<br>- Workflows using server-side tools extensively&para;<br>- Tasks that need to maintain exact state across many variables</ins></div>
        </div>

        <h2 style="margin-top: 2rem; margin-bottom: 1rem;">Unified Diff</h2>
        <pre><code>--- a/build-with-claude/context-editing.md
+++ b/build-with-claude/context-editing.md
@@ -0,0 +1,1109 @@
+# Context editing
+
+Automatically manage conversation context as it grows with context editing.
+
+---
+
+## Overview
+
+Context editing allows you to automatically manage conversation context as it grows, helping you optimize costs and stay within context window limits. You can use server-side API strategies, client-side SDK features, or both together.
+
+| Approach | Where it runs | Strategies | How it works |
+|----------|---------------|------------|--------------|
+| **Server-side** | API | Tool result clearing (`clear_tool_uses_20250919`)&lt;br/&gt;Thinking block clearing (`clear_thinking_20251015`) | Applied before the prompt reaches Claude. Clears specific content from conversation history. Each strategy can be configured independently. |
+| **Client-side** | SDK | Compaction | Available in [Python and TypeScript SDKs](/docs/en/api/client-sdks) when using [`tool_runner`](/docs/en/agents-and-tools/tool-use/implement-tool-use#tool-runner-beta). Generates a summary and replaces full conversation history. See [Compaction](#client-side-compaction-sdk) below. |
+
+## Server-side strategies
+
+&lt;Note&gt;
+Context editing is currently in beta with support for tool result clearing and thinking block clearing. To enable it, use the beta header `context-management-2025-06-27` in your API requests.
+
+Please reach out through our [feedback form](https://forms.gle/YXC2EKGMhjN1c4L88) to share your feedback on this feature.
+&lt;/Note&gt;
+
+### Tool result clearing
+
+The `clear_tool_uses_20250919` strategy clears tool results when conversation context grows beyond your configured threshold. When activated, the API automatically clears the oldest tool results in chronological order, replacing them with placeholder text to let Claude know the tool result was removed. By default, only tool results are cleared. You can optionally clear both tool results and tool calls (the tool use parameters) by setting `clear_tool_inputs` to true.
+
+### Thinking block clearing
+
+The `clear_thinking_20251015` strategy manages `thinking` blocks in conversations when extended thinking is enabled. This strategy automatically clears older thinking blocks from previous turns.
+
+&lt;Tip&gt;
+**Default behavior**: When extended thinking is enabled without configuring the `clear_thinking_20251015` strategy, the API automatically keeps only the thinking blocks from the last assistant turn (equivalent to `keep: {type: &#34;thinking_turns&#34;, value: 1}`).
+
+To maximize cache hits, preserve all thinking blocks by setting `keep: &#34;all&#34;`.
+&lt;/Tip&gt;
+
+&lt;Note&gt;
+An assistant conversation turn may include multiple content blocks (e.g. when using tools) and multiple thinking blocks (e.g. with [interleaved thinking](/docs/en/build-with-claude/extended-thinking#interleaved-thinking)).
+&lt;/Note&gt;
+
+&lt;Tip&gt;
+**Context editing happens server-side**
+
+Context editing is applied **server-side** before the prompt reaches Claude. Your client application maintains the full, unmodified conversation historyâ€”you do not need to sync your client state with the edited version. Continue managing your full conversation history locally as you normally would.
+&lt;/Tip&gt;
+
+&lt;Tip&gt;
+**Context editing and prompt caching**
+
+Context editing&#39;s interaction with [prompt caching](/docs/en/build-with-claude/prompt-caching) varies by strategy:
+
+- **Tool result clearing**: Invalidates cached prompt prefixes when content is cleared. To account for this, we recommend clearing enough tokens to make the cache invalidation worthwhile. Use the `clear_at_least` parameter to ensure a minimum number of tokens is cleared each time. You&#39;ll incur cache write costs each time content is cleared, but subsequent requests can reuse the newly cached prefix.
+
+- **Thinking block clearing**: When thinking blocks are **kept** in context (not cleared), the prompt cache is preserved, enabling cache hits and reducing input token costs. When thinking blocks are **cleared**, the cache is invalidated at the point where clearing occurs. Configure the `keep` parameter based on whether you want to prioritize cache performance or context window availability.
+&lt;/Tip&gt;
+
+## Supported models
+
+Context editing is available on:
+
+- Claude Opus 4.5 (`claude-opus-4-5-20251101`)
+- Claude Opus 4.1 (`claude-opus-4-1-20250805`)
+- Claude Opus 4 (`claude-opus-4-20250514`)
+- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
+- Claude Sonnet 4 (`claude-sonnet-4-20250514`)
+- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
+
+## Tool result clearing usage
+
+The simplest way to enable tool result clearing is to specify only the strategy type, as all other [configuration options](#configuration-options-for-tool-result-clearing) will use their default values:
+
+&lt;CodeGroup&gt;
+
+```bash cURL
+curl https://api.anthropic.com/v1/messages \
+    --header &#34;x-api-key: $ANTHROPIC_API_KEY&#34; \
+    --header &#34;anthropic-version: 2023-06-01&#34; \
+    --header &#34;content-type: application/json&#34; \
+    --header &#34;anthropic-beta: context-management-2025-06-27&#34; \
+    --data &#39;{
+        &#34;model&#34;: &#34;claude-sonnet-4-5&#34;,
+        &#34;max_tokens&#34;: 4096,
+        &#34;messages&#34;: [
+            {
+                &#34;role&#34;: &#34;user&#34;,
+                &#34;content&#34;: &#34;Search for recent developments in AI&#34;
+            }
+        ],
+        &#34;tools&#34;: [
+            {
+                &#34;type&#34;: &#34;web_search_20250305&#34;,
+                &#34;name&#34;: &#34;web_search&#34;
+            }
+        ],
+        &#34;context_management&#34;: {
+            &#34;edits&#34;: [
+                {&#34;type&#34;: &#34;clear_tool_uses_20250919&#34;}
+            ]
+        }
+    }&#39;
+```
+
+```python Python
+response = client.beta.messages.create(
+    model=&#34;claude-sonnet-4-5&#34;,
+    max_tokens=4096,
+    messages=[
+        {
+            &#34;role&#34;: &#34;user&#34;,
+            &#34;content&#34;: &#34;Search for recent developments in AI&#34;
+        }
+    ],
+    tools=[
+        {
+            &#34;type&#34;: &#34;web_search_20250305&#34;,
+            &#34;name&#34;: &#34;web_search&#34;
+        }
+    ],
+    betas=[&#34;context-management-2025-06-27&#34;],
+    context_management={
+        &#34;edits&#34;: [
+            {&#34;type&#34;: &#34;clear_tool_uses_20250919&#34;}
+        ]
+    }
+)
+```
+
+```typescript TypeScript
+import Anthropic from &#39;@anthropic-ai/sdk&#39;;
+
+const anthropic = new Anthropic({
+  apiKey: process.env.ANTHROPIC_API_KEY,
+});
+
+const response = await anthropic.beta.messages.create({
+  model: &#34;claude-sonnet-4-5&#34;,
+  max_tokens: 4096,
+  messages: [
+    {
+      role: &#34;user&#34;,
+      content: &#34;Search for recent developments in AI&#34;
+    }
+  ],
+  tools: [
+    {
+      type: &#34;web_search_20250305&#34;,
+      name: &#34;web_search&#34;
+    }
+  ],
+  context_management: {
+    edits: [
+      { type: &#34;clear_tool_uses_20250919&#34; }
+    ]
+  },
+  betas: [&#34;context-management-2025-06-27&#34;]
+});
+```
+
+&lt;/CodeGroup&gt;
+
+### Advanced configuration
+
+You can customize the tool result clearing behavior with additional parameters:
+
+&lt;CodeGroup&gt;
+
+```bash cURL
+curl https://api.anthropic.com/v1/messages \
+    --header &#34;x-api-key: $ANTHROPIC_API_KEY&#34; \
+    --header &#34;anthropic-version: 2023-06-01&#34; \
+    --header &#34;content-type: application/json&#34; \
+    --header &#34;anthropic-beta: context-management-2025-06-27&#34; \
+    --data &#39;{
+        &#34;model&#34;: &#34;claude-sonnet-4-5&#34;,
+        &#34;max_tokens&#34;: 4096,
+        &#34;messages&#34;: [
+            {
+                &#34;role&#34;: &#34;user&#34;,
+                &#34;content&#34;: &#34;Create a simple command line calculator app using Python&#34;
+            }
+        ],
+        &#34;tools&#34;: [
+            {
+                &#34;type&#34;: &#34;text_editor_20250728&#34;,
+                &#34;name&#34;: &#34;str_replace_based_edit_tool&#34;,
+                &#34;max_characters&#34;: 10000
+            },
+            {
+                &#34;type&#34;: &#34;web_search_20250305&#34;,
+                &#34;name&#34;: &#34;web_search&#34;,
+                &#34;max_uses&#34;: 3
+            }
+        ],
+        &#34;context_management&#34;: {
+            &#34;edits&#34;: [
+                {
+                    &#34;type&#34;: &#34;clear_tool_uses_20250919&#34;,
+                    &#34;trigger&#34;: {
+                        &#34;type&#34;: &#34;input_tokens&#34;,
+                        &#34;value&#34;: 30000
+                    },
+                    &#34;keep&#34;: {
+                        &#34;type&#34;: &#34;tool_uses&#34;,
+                        &#34;value&#34;: 3
+                    },
+                    &#34;clear_at_least&#34;: {
+                        &#34;type&#34;: &#34;input_tokens&#34;,
+                        &#34;value&#34;: 5000
+                    },
+                    &#34;exclude_tools&#34;: [&#34;web_search&#34;]
+                }
+            ]
+        }
+    }&#39;
+```
+
+```python Python
+response = client.beta.messages.create(
+    model=&#34;claude-sonnet-4-5&#34;,
+    max_tokens=4096,
+    messages=[
+        {
+            &#34;role&#34;: &#34;user&#34;,
+            &#34;content&#34;: &#34;Create a simple command line calculator app using Python&#34;
+        }
+    ],
+    tools=[
+        {
+            &#34;type&#34;: &#34;text_editor_20250728&#34;,
+            &#34;name&#34;: &#34;str_replace_based_edit_tool&#34;,
+            &#34;max_characters&#34;: 10000
+        },
+        {
+            &#34;type&#34;: &#34;web_search_20250305&#34;,
+            &#34;name&#34;: &#34;web_search&#34;,
+            &#34;max_uses&#34;: 3
+        }
+    ],
+    betas=[&#34;context-management-2025-06-27&#34;],
+    context_management={
+        &#34;edits&#34;: [
+            {
+                &#34;type&#34;: &#34;clear_tool_uses_20250919&#34;,
+                # Trigger clearing when threshold is exceeded
+                &#34;trigger&#34;: {
+                    &#34;type&#34;: &#34;input_tokens&#34;,
+                    &#34;value&#34;: 30000
+                },
+                # Number of tool uses to keep after clearing
+                &#34;keep&#34;: {
+                    &#34;type&#34;: &#34;tool_uses&#34;,
+                    &#34;value&#34;: 3
+                },
+                # Optional: Clear at least this many tokens
+                &#34;clear_at_least&#34;: {
+                    &#34;type&#34;: &#34;input_tokens&#34;,
+                    &#34;value&#34;: 5000
+                },
+                # Exclude these tools from being cleared
+                &#34;exclude_tools&#34;: [&#34;web_search&#34;]
+            }
+        ]
+    }
+)
+```
+
+```typescript TypeScript
+import Anthropic from &#39;@anthropic-ai/sdk&#39;;
+
+const anthropic = new Anthropic({
+  apiKey: process.env.ANTHROPIC_API_KEY,
+});
+
+const response = await anthropic.beta.messages.create({
+  model: &#34;claude-sonnet-4-5&#34;,
+  max_tokens: 4096,
+  messages: [
+    {
+      role: &#34;user&#34;,
+      content: &#34;Create a simple command line calculator app using Python&#34;
+    }
+  ],
+  tools: [
+    {
+      type: &#34;text_editor_20250728&#34;,
+      name: &#34;str_replace_based_edit_tool&#34;,
+      max_characters: 10000
+    },
+    {
+      type: &#34;web_search_20250305&#34;,
+      name: &#34;web_search&#34;,
+      max_uses: 3
+    }
+  ],
+  betas: [&#34;context-management-2025-06-27&#34;],
+  context_management: {
+    edits: [
+      {
+        type: &#34;clear_tool_uses_20250919&#34;,
+        // Trigger clearing when threshold is exceeded
+        trigger: {
+          type: &#34;input_tokens&#34;,
+          value: 30000
+        },
+        // Number of tool uses to keep after clearing
+        keep: {
+          type: &#34;tool_uses&#34;,
+          value: 3
+        },
+        // Optional: Clear at least this many tokens
+        clear_at_least: {
+          type: &#34;input_tokens&#34;,
+          value: 5000
+        },
+        // Exclude these tools from being cleared
+        exclude_tools: [&#34;web_search&#34;]
+      }
+    ]
+  }
+});
+```
+
+&lt;/CodeGroup&gt;
+
+## Thinking block clearing usage
+
+Enable thinking block clearing to manage context and prompt caching effectively when extended thinking is enabled:
+
+&lt;CodeGroup&gt;
+
+```bash cURL
+curl https://api.anthropic.com/v1/messages \
+    --header &#34;x-api-key: $ANTHROPIC_API_KEY&#34; \
+    --header &#34;anthropic-version: 2023-06-01&#34; \
+    --header &#34;content-type: application/json&#34; \
+    --header &#34;anthropic-beta: context-management-2025-06-27&#34; \
+    --data &#39;{
+        &#34;model&#34;: &#34;claude-sonnet-4-5-20250929&#34;,
+        &#34;max_tokens&#34;: 1024,
+        &#34;messages&#34;: [...],
+        &#34;thinking&#34;: {
+            &#34;type&#34;: &#34;enabled&#34;,
+            &#34;budget_tokens&#34;: 10000
+        },
+        &#34;context_management&#34;: {
+            &#34;edits&#34;: [
+                {
+                    &#34;type&#34;: &#34;clear_thinking_20251015&#34;,
+                    &#34;keep&#34;: {
+                        &#34;type&#34;: &#34;thinking_turns&#34;,
+                        &#34;value&#34;: 2
+                    }
+                }
+            ]
+        }
+    }&#39;
+```
+
+```python Python
+response = client.beta.messages.create(
+    model=&#34;claude-sonnet-4-5-20250929&#34;,
+    max_tokens=1024,
+    messages=[...],
+    thinking={
+        &#34;type&#34;: &#34;enabled&#34;,
+        &#34;budget_tokens&#34;: 10000
+    },
+    betas=[&#34;context-management-2025-06-27&#34;],
+    context_management={
+        &#34;edits&#34;: [
+            {
+                &#34;type&#34;: &#34;clear_thinking_20251015&#34;,
+                &#34;keep&#34;: {
+                    &#34;type&#34;: &#34;thinking_turns&#34;,
+                    &#34;value&#34;: 2
+                }
+            }
+        ]
+    }
+)
+```
+
+```typescript TypeScript
+import Anthropic from &#39;@anthropic-ai/sdk&#39;;
+
+const anthropic = new Anthropic({
+  apiKey: process.env.ANTHROPIC_API_KEY,
+});
+
+const response = await anthropic.beta.messages.create({
+  model: &#34;claude-sonnet-4-5-20250929&#34;,
+  max_tokens: 1024,
+  messages: [...],
+  thinking: {
+    type: &#34;enabled&#34;,
+    budget_tokens: 10000
+  },
+  betas: [&#34;context-management-2025-06-27&#34;],
+  context_management: {
+    edits: [
+      {
+        type: &#34;clear_thinking_20251015&#34;,
+        keep: {
+          type: &#34;thinking_turns&#34;,
+          value: 2
+        }
+      }
+    ]
+  }
+});
+```
+
+&lt;/CodeGroup&gt;
+
+### Configuration options for thinking block clearing
+
+The `clear_thinking_20251015` strategy supports the following configuration:
+
+| Configuration option | Default | Description |
+|---------------------|---------|-------------|
+| `keep` | `{type: &#34;thinking_turns&#34;, value: 1}` | Defines how many recent assistant turns with thinking blocks to preserve. Use `{type: &#34;thinking_turns&#34;, value: N}` where N must be &gt; 0 to keep the last N turns, or `&#34;all&#34;` to keep all thinking blocks. |
+
+**Example configurations:**
+
+```json
+// Keep thinking blocks from the last 3 assistant turns
+{
+  &#34;type&#34;: &#34;clear_thinking_20251015&#34;,
+  &#34;keep&#34;: {
+    &#34;type&#34;: &#34;thinking_turns&#34;,
+    &#34;value&#34;: 3
+  }
+}
+
+// Keep all thinking blocks (maximizes cache hits)
+{
+  &#34;type&#34;: &#34;clear_thinking_20251015&#34;,
+  &#34;keep&#34;: &#34;all&#34;
+}
+```
+
+### Combining strategies
+
+You can use both thinking block clearing and tool result clearing together:
+
+&lt;Note&gt;
+When using multiple strategies, the `clear_thinking_20251015` strategy must be listed first in the `edits` array.
+&lt;/Note&gt;
+
+&lt;CodeGroup&gt;
+
+```python Python
+response = client.beta.messages.create(
+    model=&#34;claude-sonnet-4-5-20250929&#34;,
+    max_tokens=1024,
+    messages=[...],
+    thinking={
+        &#34;type&#34;: &#34;enabled&#34;,
+        &#34;budget_tokens&#34;: 10000
+    },
+    tools=[...],
+    betas=[&#34;context-management-2025-06-27&#34;],
+    context_management={
+        &#34;edits&#34;: [
+            {
+                &#34;type&#34;: &#34;clear_thinking_20251015&#34;,
+                &#34;keep&#34;: {
+                    &#34;type&#34;: &#34;thinking_turns&#34;,
+                    &#34;value&#34;: 2
+                }
+            },
+            {
+                &#34;type&#34;: &#34;clear_tool_uses_20250919&#34;,
+                &#34;trigger&#34;: {
+                    &#34;type&#34;: &#34;input_tokens&#34;,
+                    &#34;value&#34;: 50000
+                },
+                &#34;keep&#34;: {
+                    &#34;type&#34;: &#34;tool_uses&#34;,
+                    &#34;value&#34;: 5
+                }
+            }
+        ]
+    }
+)
+```
+
+```typescript TypeScript
+const response = await anthropic.beta.messages.create({
+  model: &#34;claude-sonnet-4-5-20250929&#34;,
+  max_tokens: 1024,
+  messages: [...],
+  thinking: {
+    type: &#34;enabled&#34;,
+    budget_tokens: 10000
+  },
+  tools: [...],
+  betas: [&#34;context-management-2025-06-27&#34;],
+  context_management: {
+    edits: [
+      {
+        type: &#34;clear_thinking_20251015&#34;,
+        keep: {
+          type: &#34;thinking_turns&#34;,
+          value: 2
+        }
+      },
+      {
+        type: &#34;clear_tool_uses_20250919&#34;,
+        trigger: {
+          type: &#34;input_tokens&#34;,
+          value: 50000
+        },
+        keep: {
+          type: &#34;tool_uses&#34;,
+          value: 5
+        }
+      }
+    ]
+  }
+});
+```
+
+&lt;/CodeGroup&gt;
+
+## Configuration options for tool result clearing
+
+| Configuration option | Default | Description |
+|---------------------|---------|-------------|
+| `trigger` | 100,000 input tokens | Defines when the context editing strategy activates. Once the prompt exceeds this threshold, clearing will begin. You can specify this value in either `input_tokens` or `tool_uses`. |
+| `keep` | 3 tool uses | Defines how many recent tool use/result pairs to keep after clearing occurs. The API removes the oldest tool interactions first, preserving the most recent ones. |
+| `clear_at_least` | None | Ensures a minimum number of tokens is cleared each time the strategy activates. If the API can&#39;t clear at least the specified amount, the strategy will not be applied. This helps determine if context clearing is worth breaking your prompt cache. |
+| `exclude_tools` | None | List of tool names whose tool uses and results should never be cleared. Useful for preserving important context. |
+| `clear_tool_inputs` | `false` | Controls whether the tool call parameters are cleared along with the tool results. By default, only the tool results are cleared while keeping Claude&#39;s original tool calls visible. |
+
+## Context editing response
+
+You can see which context edits were applied to your request using the `context_management` response field, along with helpful statistics about the content and input tokens cleared.
+
+```json Response
+{
+    &#34;id&#34;: &#34;msg_013Zva2CMHLNnXjNJJKqJ2EF&#34;,
+    &#34;type&#34;: &#34;message&#34;,
+    &#34;role&#34;: &#34;assistant&#34;,
+    &#34;content&#34;: [...],
+    &#34;usage&#34;: {...},
+    &#34;context_management&#34;: {
+        &#34;applied_edits&#34;: [
+            // When using `clear_thinking_20251015`
+            {
+                &#34;type&#34;: &#34;clear_thinking_20251015&#34;,
+                &#34;cleared_thinking_turns&#34;: 3,
+                &#34;cleared_input_tokens&#34;: 15000
+            },
+            // When using `clear_tool_uses_20250919`
+            {
+                &#34;type&#34;: &#34;clear_tool_uses_20250919&#34;,
+                &#34;cleared_tool_uses&#34;: 8,
+                &#34;cleared_input_tokens&#34;: 50000
+            }
+        ]
+    }
+}
+```
+
+For streaming responses, the context edits will be included in the final `message_delta` event:
+
+```json Streaming Response
+{
+    &#34;type&#34;: &#34;message_delta&#34;,
+    &#34;delta&#34;: {
+        &#34;stop_reason&#34;: &#34;end_turn&#34;,
+        &#34;stop_sequence&#34;: null
+    },
+    &#34;usage&#34;: {
+        &#34;output_tokens&#34;: 1024
+    },
+    &#34;context_management&#34;: {
+        &#34;applied_edits&#34;: [...]
+    }
+}
+```
+
+## Token counting
+
+The [token counting](/docs/en/build-with-claude/token-counting) endpoint supports context management, allowing you to preview how many tokens your prompt will use after context editing is applied.
+
+&lt;CodeGroup&gt;
+
+```bash cURL
+curl https://api.anthropic.com/v1/messages/count_tokens \
+    --header &#34;x-api-key: $ANTHROPIC_API_KEY&#34; \
+    --header &#34;anthropic-version: 2023-06-01&#34; \
+    --header &#34;content-type: application/json&#34; \
+    --header &#34;anthropic-beta: context-management-2025-06-27&#34; \
+    --data &#39;{
+        &#34;model&#34;: &#34;claude-sonnet-4-5&#34;,
+        &#34;messages&#34;: [
+            {
+                &#34;role&#34;: &#34;user&#34;,
+                &#34;content&#34;: &#34;Continue our conversation...&#34;
+            }
+        ],
+        &#34;tools&#34;: [...],
+        &#34;context_management&#34;: {
+            &#34;edits&#34;: [
+                {
+                    &#34;type&#34;: &#34;clear_tool_uses_20250919&#34;,
+                    &#34;trigger&#34;: {
+                        &#34;type&#34;: &#34;input_tokens&#34;,
+                        &#34;value&#34;: 30000
+                    },
+                    &#34;keep&#34;: {
+                        &#34;type&#34;: &#34;tool_uses&#34;,
+                        &#34;value&#34;: 5
+                    }
+                }
+            ]
+        }
+    }&#39;
+```
+
+```python Python
+response = client.beta.messages.count_tokens(
+    model=&#34;claude-sonnet-4-5&#34;,
+    messages=[
+        {
+            &#34;role&#34;: &#34;user&#34;,
+            &#34;content&#34;: &#34;Continue our conversation...&#34;
+        }
+    ],
+    tools=[...],  # Your tool definitions
+    betas=[&#34;context-management-2025-06-27&#34;],
+    context_management={
+        &#34;edits&#34;: [
+            {
+                &#34;type&#34;: &#34;clear_tool_uses_20250919&#34;,
+                &#34;trigger&#34;: {
+                    &#34;type&#34;: &#34;input_tokens&#34;,
+                    &#34;value&#34;: 30000
+                },
+                &#34;keep&#34;: {
+                    &#34;type&#34;: &#34;tool_uses&#34;,
+                    &#34;value&#34;: 5
+                }
+            }
+        ]
+    }
+)
+
+print(f&#34;Original tokens: {response.context_management[&#39;original_input_tokens&#39;]}&#34;)
+print(f&#34;After clearing: {response.input_tokens}&#34;)
+print(f&#34;Savings: {response.context_management[&#39;original_input_tokens&#39;] - response.input_tokens} tokens&#34;)
+```
+
+```typescript TypeScript
+import Anthropic from &#39;@anthropic-ai/sdk&#39;;
+
+const anthropic = new Anthropic({
+  apiKey: process.env.ANTHROPIC_API_KEY,
+});
+
+const response = await anthropic.beta.messages.countTokens({
+  model: &#34;claude-sonnet-4-5&#34;,
+  messages: [
+    {
+      role: &#34;user&#34;,
+      content: &#34;Continue our conversation...&#34;
+    }
+  ],
+  tools: [...],  // Your tool definitions
+  betas: [&#34;context-management-2025-06-27&#34;],
+  context_management: {
+    edits: [
+      {
+        type: &#34;clear_tool_uses_20250919&#34;,
+        trigger: {
+          type: &#34;input_tokens&#34;,
+          value: 30000
+        },
+        keep: {
+          type: &#34;tool_uses&#34;,
+          value: 5
+        }
+      }
+    ]
+  }
+});
+
+console.log(`Original tokens: ${response.context_management?.original_input_tokens}`);
+console.log(`After clearing: ${response.input_tokens}`);
+console.log(`Savings: ${(response.context_management?.original_input_tokens || 0) - response.input_tokens} tokens`);
+```
+&lt;/CodeGroup&gt;
+
+```json Response
+{
+    &#34;input_tokens&#34;: 25000,
+    &#34;context_management&#34;: {
+        &#34;original_input_tokens&#34;: 70000
+    }
+}
+```
+
+The response shows both the final token count after context management is applied (`input_tokens`) and the original token count before any clearing occurred (`original_input_tokens`).
+
+## Using with the Memory Tool
+
+Context editing can be combined with the [memory tool](/docs/en/agents-and-tools/tool-use/memory-tool). When your conversation context approaches the configured clearing threshold, Claude receives an automatic warning to preserve important information. This enables Claude to save tool results or context to its memory files before they&#39;re cleared from the conversation history.
+
+This combination allows you to:
+
+- **Preserve important context**: Claude can write essential information from tool results to memory files before those results are cleared
+- **Maintain long-running workflows**: Enable agentic workflows that would otherwise exceed context limits by offloading information to persistent storage
+- **Access information on demand**: Claude can look up previously cleared information from memory files when needed, rather than keeping everything in the active context window
+
+For example, in a file editing workflow where Claude performs many operations, Claude can summarize completed changes to memory files as the context grows. When tool results are cleared, Claude retains access to that information through its memory system and can continue working effectively.
+
+To use both features together, enable them in your API request:
+
+&lt;CodeGroup&gt;
+
+```python Python
+response = client.beta.messages.create(
+    model=&#34;claude-sonnet-4-5&#34;,
+    max_tokens=4096,
+    messages=[...],
+    tools=[
+        {
+            &#34;type&#34;: &#34;memory_20250818&#34;,
+            &#34;name&#34;: &#34;memory&#34;
+        },
+        # Your other tools
+    ],
+    betas=[&#34;context-management-2025-06-27&#34;],
+    context_management={
+        &#34;edits&#34;: [
+            {&#34;type&#34;: &#34;clear_tool_uses_20250919&#34;}
+        ]
+    }
+)
+```
+
+```typescript TypeScript
+import Anthropic from &#39;@anthropic-ai/sdk&#39;;
+
+const anthropic = new Anthropic({
+  apiKey: process.env.ANTHROPIC_API_KEY,
+});
+
+const response = await anthropic.beta.messages.create({
+  model: &#34;claude-sonnet-4-5&#34;,
+  max_tokens: 4096,
+  messages: [...],
+  tools: [
+    {
+      type: &#34;memory_20250818&#34;,
+      name: &#34;memory&#34;
+    },
+    // Your other tools
+  ],
+  betas: [&#34;context-management-2025-06-27&#34;],
+  context_management: {
+    edits: [
+      { type: &#34;clear_tool_uses_20250919&#34; }
+    ]
+  }
+});
+```
+
+&lt;/CodeGroup&gt;
+
+## Client-side compaction (SDK)
+
+&lt;Note&gt;
+Compaction is available in the [Python and TypeScript SDKs](/docs/en/api/client-sdks) when using the [`tool_runner` method](/docs/en/agents-and-tools/tool-use/implement-tool-use#tool-runner-beta).
+&lt;/Note&gt;
+
+Compaction is an SDK feature that automatically manages conversation context by generating summaries when token usage grows too large. Unlike server-side context editing strategies that clear content, compaction instructs Claude to summarize the conversation history, then replaces the full history with that summary. This allows Claude to continue working on long-running tasks that would otherwise exceed the [context window](/docs/en/build-with-claude/context-windows).
+
+### How compaction works
+
+When compaction is enabled, the SDK monitors token usage after each model response:
+
+1. **Threshold check**: The SDK calculates total tokens as `input_tokens + cache_creation_input_tokens + cache_read_input_tokens + output_tokens`
+2. **Summary generation**: When the threshold is exceeded, a summary prompt is injected as a user turn, and Claude generates a structured summary wrapped in `&lt;summary&gt;&lt;/summary&gt;` tags
+3. **Context replacement**: The SDK extracts the summary and replaces the entire message history with it
+4. **Continuation**: The conversation resumes from the summary, with Claude picking up where it left off
+
+### Using compaction
+
+Add `compaction_control` to your `tool_runner` call:
+
+&lt;CodeGroup&gt;
+
+```python Python
+import anthropic
+
+client = anthropic.Anthropic()
+
+runner = client.beta.messages.tool_runner(
+    model=&#34;claude-sonnet-4-5&#34;,
+    max_tokens=4096,
+    tools=[...],
+    messages=[
+        {
+            &#34;role&#34;: &#34;user&#34;,
+            &#34;content&#34;: &#34;Analyze all the files in this directory and write a summary report.&#34;
+        }
+    ],
+    compaction_control={
+        &#34;enabled&#34;: True,
+        &#34;context_token_threshold&#34;: 100000
+    }
+)
+
+for message in runner:
+    print(f&#34;Tokens used: {message.usage.input_tokens}&#34;)
+
+final = runner.until_done()
+```
+
+```typescript TypeScript
+import Anthropic from &#39;@anthropic-ai/sdk&#39;;
+
+const client = new Anthropic();
+
+const runner = client.beta.messages.toolRunner({
+    model: &#39;claude-sonnet-4-5&#39;,
+    max_tokens: 4096,
+    tools: [...],
+    messages: [
+        {
+            role: &#39;user&#39;,
+            content: &#39;Analyze all the files in this directory and write a summary report.&#39;
+        }
+    ],
+    compactionControl: {
+        enabled: true,
+        contextTokenThreshold: 100000
+    }
+});
+
+for await (const message of runner) {
+    console.log(&#39;Tokens used:&#39;, message.usage.input_tokens);
+}
+
+const finalMessage = await runner.runUntilDone();
+```
+
+&lt;/CodeGroup&gt;
+
+#### What happens during compaction
+
+As the conversation grows, the message history accumulates:
+
+**Before compaction (approaching 100k tokens):**
+```json
+[
+  { &#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Analyze all files and write a report...&#34; },
+  { &#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;I&#39;ll help. Let me start by reading...&#34; },
+  { &#34;role&#34;: &#34;user&#34;, &#34;content&#34;: [{ &#34;type&#34;: &#34;tool_result&#34;, &#34;tool_use_id&#34;: &#34;...&#34;, &#34;content&#34;: &#34;...&#34; }] },
+  { &#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;Based on file1.txt, I see...&#34; },
+  { &#34;role&#34;: &#34;user&#34;, &#34;content&#34;: [{ &#34;type&#34;: &#34;tool_result&#34;, &#34;tool_use_id&#34;: &#34;...&#34;, &#34;content&#34;: &#34;...&#34; }] },
+  { &#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;After analyzing file2.txt...&#34; },
+  // ... 50 more exchanges like this ...
+]
+```
+
+When tokens exceed the threshold, the SDK injects a summary request and Claude generates a summary. The entire history is then replaced:
+
+**After compaction (back to ~2-3k tokens):**
+```json
+[
+  {
+    &#34;role&#34;: &#34;assistant&#34;,
+    &#34;content&#34;: &#34;# Task Overview\nThe user requested analysis of directory files to produce a summary report...\n\n# Current State\nAnalyzed 52 files across 3 subdirectories. Key findings documented in report.md...\n\n# Important Discoveries\n- Configuration files use YAML format\n- Found 3 deprecated dependencies\n- Test coverage at 67%\n\n# Next Steps\n1. Analyze remaining files in /src/legacy\n2. Complete final report sections...\n\n# Context to Preserve\nUser prefers markdown format with executive summary first...&#34;
+  }
+]
+```
+
+Claude continues working from this summary as if it were the original conversation history.
+
+### Configuration options
+
+| Parameter | Type | Required | Default | Description |
+|-----------|------|----------|---------|-------------|
+| `enabled` | boolean | Yes | - | Whether to enable automatic compaction |
+| `context_token_threshold` | number | No | 100,000 | Token count at which compaction triggers |
+| `model` | string | No | Same as main model | Model to use for generating summaries |
+| `summary_prompt` | string | No | See below | Custom prompt for summary generation |
+
+#### Choosing a token threshold
+
+The threshold determines when compaction occurs. A lower threshold means more frequent compactions with smaller context windows. A higher threshold allows more context but risks hitting limits.
+
+&lt;CodeGroup&gt;
+
+```python Python
+# More frequent compaction for memory-constrained scenarios
+compaction_control={
+    &#34;enabled&#34;: True,
+    &#34;context_token_threshold&#34;: 50000
+}
+
+# Less frequent compaction when you need more context
+compaction_control={
+    &#34;enabled&#34;: True,
+    &#34;context_token_threshold&#34;: 150000
+}
+```
+
+```typescript TypeScript
+// More frequent compaction for memory-constrained scenarios
+compactionControl: {
+    enabled: true,
+    contextTokenThreshold: 50000
+}
+
+// Less frequent compaction when you need more context
+compactionControl: {
+    enabled: true,
+    contextTokenThreshold: 150000
+}
+```
+
+&lt;/CodeGroup&gt;
+
+#### Using a different model for summaries
+
+You can use a faster or cheaper model for generating summaries:
+
+&lt;CodeGroup&gt;
+
+```python Python
+compaction_control={
+    &#34;enabled&#34;: True,
+    &#34;context_token_threshold&#34;: 100000,
+    &#34;model&#34;: &#34;claude-haiku-4-5&#34;
+}
+```
+
+```typescript TypeScript
+compactionControl: {
+    enabled: true,
+    contextTokenThreshold: 100000,
+    model: &#39;claude-haiku-4-5&#39;
+}
+```
+
+&lt;/CodeGroup&gt;
+
+#### Custom summary prompts
+
+You can provide a custom prompt for domain-specific needs. Your prompt should instruct Claude to wrap its summary in `&lt;summary&gt;&lt;/summary&gt;` tags.
+
+&lt;CodeGroup&gt;
+
+```python Python
+compaction_control={
+    &#34;enabled&#34;: True,
+    &#34;context_token_threshold&#34;: 100000,
+    &#34;summary_prompt&#34;: &#34;&#34;&#34;Summarize the research conducted so far, including:
+- Sources consulted and key findings
+- Questions answered and remaining unknowns
+- Recommended next steps
+
+Wrap your summary in &lt;summary&gt;&lt;/summary&gt; tags.&#34;&#34;&#34;
+}
+```
+
+```typescript TypeScript
+compactionControl: {
+    enabled: true,
+    contextTokenThreshold: 100000,
+    summaryPrompt: `Summarize the research conducted so far, including:
+- Sources consulted and key findings
+- Questions answered and remaining unknowns
+- Recommended next steps
+
+Wrap your summary in &lt;summary&gt;&lt;/summary&gt; tags.`
+}
+```
+
+&lt;/CodeGroup&gt;
+
+### Default summary prompt
+
+The built-in summary prompt instructs Claude to create a structured continuation summary including:
+
+1. **Task Overview**: The user&#39;s core request, success criteria, and constraints
+2. **Current State**: What has been completed, files modified, and artifacts produced
+3. **Important Discoveries**: Technical constraints, decisions made, errors resolved, and failed approaches
+4. **Next Steps**: Specific actions needed, blockers, and priority order
+5. **Context to Preserve**: User preferences, domain-specific details, and commitments made
+
+This structure enables Claude to resume work efficiently without losing important context or repeating mistakes.
+
+&lt;section title=&#34;View full default prompt&#34;&gt;
+
+```
+You have been working on the task described above but have not yet completed it. Write a continuation summary that will allow you (or another instance of yourself) to resume work efficiently in a future context window where the conversation history will be replaced with this summary. Your summary should be structured, concise, and actionable. Include:
+
+1. Task Overview
+The user&#39;s core request and success criteria
+Any clarifications or constraints they specified
+
+2. Current State
+What has been completed so far
+Files created, modified, or analyzed (with paths if relevant)
+Key outputs or artifacts produced
+
+3. Important Discoveries
+Technical constraints or requirements uncovered
+Decisions made and their rationale
+Errors encountered and how they were resolved
+What approaches were tried that didn&#39;t work (and why)
+
+4. Next Steps
+Specific actions needed to complete the task
+Any blockers or open questions to resolve
+Priority order if multiple steps remain
+
+5. Context to Preserve
+User preferences or style requirements
+Domain-specific details that aren&#39;t obvious
+Any promises made to the user
+
+Be concise but completeâ€”err on the side of including information that would prevent duplicate work or repeated mistakes. Write in a way that enables immediate resumption of the task.
+
+Wrap your summary in &lt;summary&gt;&lt;/summary&gt; tags.
+```
+
+&lt;/section&gt;
+
+### Limitations
+
+#### Server-side tools
+
+&lt;Warning&gt;
+Compaction requires special consideration when using server-side tools such as [web search](/docs/en/agents-and-tools/tool-use/web-search-tool) or [web fetch](/docs/en/agents-and-tools/tool-use/web-fetch-tool).
+&lt;/Warning&gt;
+
+When using server-side tools, the SDK may incorrectly calculate token usage, causing compaction to trigger at the wrong time.
+
+For example, after a web search operation, the API response might show:
+
+```json
+{
+  &#34;usage&#34;: {
+    &#34;input_tokens&#34;: 63000,
+    &#34;cache_read_input_tokens&#34;: 270000,
+    &#34;output_tokens&#34;: 1400
+  }
+}
+```
+
+The SDK calculates total usage as 63,000 + 270,000 = 333,000 tokens. However, the `cache_read_input_tokens` value includes accumulated reads from multiple internal API calls made by the server-side tool, not your actual conversation context. Your real context length might only be the 63,000 `input_tokens`, but the SDK sees 333k and triggers compaction prematurely.
+
+**Workarounds:**
+
+- Use the [token counting](/docs/en/build-with-claude/token-counting) endpoint to get accurate context length
+- Avoid compaction when using server-side tools extensively
+
+#### Tool use edge cases
+
+When compaction is triggered while a tool use response is pending, the SDK removes the tool use block from the message history before generating the summary. Claude will re-issue the tool call after resuming from the summary if still needed.
+
+### Monitoring compaction
+
+Enable logging to track when compaction occurs:
+
+&lt;CodeGroup&gt;
+
+```python Python
+import logging
+
+logging.basicConfig(level=logging.INFO)
+logging.getLogger(&#34;anthropic.lib.tools&#34;).setLevel(logging.INFO)
+
+# Logs will show:
+# INFO: Token usage 105000 has exceeded the threshold of 100000. Performing compaction.
+# INFO: Compaction complete. New token usage: 2500
+```
+
+```typescript TypeScript
+// The SDK logs compaction events to the console
+// You&#39;ll see messages like:
+// Token usage 105000 has exceeded the threshold of 100000. Performing compaction.
+// Compaction complete. New token usage: 2500
+```
+
+&lt;/CodeGroup&gt;
+
+### When to use compaction
+
+**Good use cases:**
+
+- Long-running agent tasks that process many files or data sources
+- Research workflows that accumulate large amounts of information
+- Multi-step tasks with clear, measurable progress
+- Tasks that produce artifacts (files, reports) that persist outside the conversation
+
+**Less ideal use cases:**
+
+- Tasks requiring precise recall of early conversation details
+- Workflows using server-side tools extensively
+- Tasks that need to maintain exact state across many variables</code></pre>
    </div>
</body>
</html>