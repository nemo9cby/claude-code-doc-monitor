<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>test-and-evaluate/develop-tests - Diff Report</title>
    <link rel="stylesheet" href="../../css/diff.css">
    <style>
        :root {
            --bg-color: #1a1a2e;
            --card-bg: #16213e;
            --text-color: #eee;
            --accent: #0f3460;
            --add-bg: #1a4d1a;
            --del-bg: #4d1a1a;
            --add-text: #4ade80;
            --del-text: #f87171;
        }
        @media (prefers-color-scheme: light) {
            :root {
                --bg-color: #f5f5f5;
                --card-bg: #fff;
                --text-color: #333;
                --accent: #e0e0e0;
                --add-bg: #d4edda;
                --del-bg: #f8d7da;
                --add-text: #155724;
                --del-text: #721c24;
            }
        }
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: var(--bg-color);
            color: var(--text-color);
            line-height: 1.6;
            padding: 2rem;
        }
        .container { max-width: 1200px; margin: 0 auto; }
        header { margin-bottom: 2rem; }
        h1 { font-size: 1.5rem; margin-bottom: 0.5rem; }
        .meta { color: #888; font-size: 0.9rem; }
        .summary {
            background: var(--card-bg);
            padding: 1rem;
            border-radius: 8px;
            margin-bottom: 2rem;
            display: flex;
            gap: 2rem;
        }
        .stat { display: flex; align-items: center; gap: 0.5rem; }
        .stat.added { color: var(--add-text); }
        .stat.removed { color: var(--del-text); }
        .analysis {
            background: var(--card-bg);
            padding: 1.5rem;
            border-radius: 8px;
            margin-bottom: 2rem;
            border-left: 4px solid #8b5cf6;
        }
        .analysis-header {
            font-weight: 600;
            margin-bottom: 0.75rem;
            color: #8b5cf6;
        }
        .analysis-content {
            white-space: pre-wrap;
            font-size: 0.95rem;
            line-height: 1.7;
        }
        .diff-container {
            background: var(--card-bg);
            border-radius: 8px;
            overflow: hidden;
        }
        .diff-header {
            background: var(--accent);
            padding: 0.75rem 1rem;
            font-weight: 600;
        }
        .diff-content {
            padding: 1rem;
            overflow-x: auto;
        }
        .diff-content ins {
            background: var(--add-bg);
            color: var(--add-text);
            text-decoration: none;
            padding: 0.1em 0.2em;
            border-radius: 2px;
        }
        .diff-content del {
            background: var(--del-bg);
            color: var(--del-text);
            text-decoration: line-through;
            padding: 0.1em 0.2em;
            border-radius: 2px;
        }
        pre {
            background: var(--accent);
            padding: 1rem;
            border-radius: 8px;
            overflow-x: auto;
            font-size: 0.85rem;
            margin-top: 2rem;
        }
        a { color: #60a5fa; }
        .back-link { margin-bottom: 1rem; display: inline-block; }
    </style>
</head>
<body>
    <div class="container">
        <a href="../../index.html" class="back-link">&larr; Back to daily report</a>

        <header>
            <h1>test-and-evaluate/develop-tests.md</h1>
            <p class="meta">Changed on 2026-02-11 10:56:44 EST</p>
        </header>

        <div class="summary">
            <div class="stat added">
                <span>+112</span> lines added
            </div>
            <div class="stat removed">
                <span>-52</span> lines removed
            </div>
        </div>

        

        <div class="diff-container">
            <div class="diff-header">Visual Diff</div>
            <div class="diff-content"><span># Create strong empirical evaluations&para;<br>&para;<br>---&para;<br>&para;<br>After defining your success criteria, the next step is designing evaluations to measure LLM performance against those criteria. This is a vital part of the prompt engineering cycle.&para;<br>&para;<br>![Flowchart of prompt engineering: test cases, preliminary prompt, iterative testing and refinement, final validation, ship](/docs/images/how-to-prompt-eng.png)&para;<br>&para;<br>This guide focuses on how to develop your test cases.&para;<br>&para;<br>## Building evals and test cases&para;<br>&para;<br>### Eval design principles&para;<br>&para;<br>1. **Be task-specific**: Design evals that mirror your real-world task distribution. Don't forget to factor in edge cases!&para;<br>    &lt;section title="Example edge cases"&gt;&para;<br>&para;<br>       - Irrelevant or nonexistent input data&para;<br>       - Overly long input data or user input&para;<br>       - [Chat use cases] Poor, harmful, or irrelevant user input&para;<br>       - Ambiguous test cases where even humans would find it hard to reach an assessment consensus&para;<br>    &para;<br>&lt;/section&gt;&para;<br>2. **Automate when possible**: Structure questions to allow for automated grading (e.g., multiple-choice, string match, code-graded, LLM-graded).&para;<br>3. **Prioritize volume over quality**: More questions with slightly lower signal automated grading is better than fewer questions with high-quality human hand-graded evals.&para;<br>&para;<br>### Example evals&para;<br>&para;<br>  &lt;section title="Task fidelity (sentiment analysis) - exact match evaluation"&gt;&para;<br>&para;<br>    **What it measures**: Exact match evals measure whether the model's output exactly matches a predefined correct answer. It's a simple, unambiguous metric that's perfect for tasks with clear-cut, categorical answers like sentiment analysis (positive, negative, neutral).&para;<br>&para;<br>    **Example eval test cases**: 1000 tweets with human-labeled sentiments.&para;<br>    ```python&para;<br>    import anthropic&para;<br></span><del style="background:#ffe6e6;">    </del><span>&para;<br>    tweets = [&para;<br>        {"text": "This movie was a total waste of time. ðŸ‘Ž", "sentiment": "negative"},&para;<br>        {"text": "The new album is ðŸ”¥! Been on repeat all day.", "sentiment": "positive"},&para;<br>        {</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>"text": "I just love it when my flight gets delayed for 5 hours. #bestdayever",</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> "sentiment": "negative"</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>},  # Edge case: Sarcasm&para;<br>        {</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>"text": "The movie's plot was terrible, but the acting was phenomenal.",</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> "sentiment": "mixed"</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>},  # Edge case: Mixed sentiment&para;<br>        # ... 996 more tweets&para;<br>    ]&para;<br>&para;<br>    client = anthropic.Anthropic()&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    def get_completion(prompt: str):&para;<br>        message = client.messages.create(&para;<br>            model="claude-opus-4-6",&para;<br>            max_tokens=50,&para;<br>            messages=[</span><del style="background:#ffe6e6;">&para;<br>            </del><span>{"role": "user", "content": prompt}</span><del style="background:#ffe6e6;">&para;<br>            </del><span>]</span><ins style="background:#e6ffe6;">,</ins><span>&para;<br>        )&para;<br>        return message.content[0].text&para;<br></span><del style="background:#ffe6e6;">    </del><ins style="background:#e6ffe6;">&para;<br></ins><span>&para;<br>    def evaluate_exact_match(model_output, correct_answer):&para;<br>        return model_output.strip().lower() == correct_answer.lower()&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    outputs = [</span><ins style="background:#e6ffe6;">&para;<br>        </ins><span>get_completion(</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>f"Classify this as 'positive', 'negative', 'neutral', or 'mixed': {tweet['text']}"</span><del style="background:#ffe6e6;">)</del><ins style="background:#e6ffe6;">&para;<br>        )&para;<br>       </ins><span> for tweet in tweets</span><ins style="background:#e6ffe6;">&para;<br>    </ins><span>]&para;<br>    accuracy = sum(</span><ins style="background:#e6ffe6;">&para;<br>        </ins><span>evaluate_exact_match(output, tweet[</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>sentiment</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>])</span><ins style="background:#e6ffe6;">&para;<br>       </ins><span> for output, tweet in zip(outputs, tweets)</span><ins style="background:#e6ffe6;">&para;<br>    </ins><span>) / len(tweets)&para;<br>    print(f"Sentiment Analysis Accuracy: {accuracy * 100}%")&para;<br>    ```&para;<br>  &para;<br>&lt;/section&gt;&para;<br>&para;<br>  &lt;section title="Consistency (FAQ bot) - cosine similarity evaluation"&gt;&para;<br>&para;<br>    **What it measures**: Cosine similarity measures the similarity between two vectors (in this case, sentence embeddings of the model's output using SBERT) by computing the cosine of the angle between them. Values closer to 1 indicate higher similarity. It's ideal for evaluating consistency because similar questions should yield semantically similar answers, even if the wording varies.&para;<br>&para;<br>    **Example eval test cases**: 50 groups with a few paraphrased versions each.&para;<br>    ```python&para;<br>    from sentence_transformers import SentenceTransformer&para;<br>    import numpy as np&para;<br>    import anthropic&para;<br></span><del style="background:#ffe6e6;">    </del><span>&para;<br>    faq_variations = [&para;<br>        {</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>"questions": [</span><ins style="background:#e6ffe6;">&para;<br>                </ins><span>"What's your return policy?",</span><ins style="background:#e6ffe6;">&para;<br>               </ins><span> "How can I return an item?",</span><ins style="background:#e6ffe6;">&para;<br>               </ins><span> "Wut's yur retrn polcy?"</span><del style="background:#ffe6e6;">]</del><span>,</span><ins style="background:#e6ffe6;">&para;<br>            ],&para;<br>           </ins><span> "answer": "Our return policy allows..."</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>},  # Edge case: Typos&para;<br>        {</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>"questions": [</span><ins style="background:#e6ffe6;">&para;<br>                </ins><span>"I bought something last week, and it's not really what I expected, so I was wondering if maybe I could possibly return it?",</span><ins style="background:#e6ffe6;">&para;<br>               </ins><span> "I read online that your policy is 30 days but that seems like it might be out of date because the website was updated six months ago, so I'm wondering what exactly is your current policy?"</span><del style="background:#ffe6e6;">]</del><span>,</span><ins style="background:#e6ffe6;">&para;<br>            ],&para;<br>           </ins><span> "answer": "Our return policy allows..."</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>},  # Edge case: Long, rambling question&para;<br>        {</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>"questions": [</span><ins style="background:#e6ffe6;">&para;<br>                </ins><span>"I'm Jane's cousin, and she said you guys have great customer service. Can I return this?",</span><ins style="background:#e6ffe6;">&para;<br>               </ins><span> "Reddit told me that contacting customer service this way was the fastest way to get an answer. I hope they're right! What is the return window for a jacket?"</span><del style="background:#ffe6e6;">]</del><span>,</span><ins style="background:#e6ffe6;">&para;<br>            ],&para;<br>           </ins><span> "answer": "Our return policy allows..."</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>},  # Edge case: Irrelevant info&para;<br>        # ... 47 more FAQs&para;<br>    ]&para;<br>&para;<br>    client = anthropic.Anthropic()&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    def get_completion(prompt: str):&para;<br>        message = client.messages.create(&para;<br>            model="claude-opus-4-6",&para;<br>            max_tokens=2048,&para;<br>            messages=[</span><del style="background:#ffe6e6;">&para;<br>            </del><span>{"role": "user", "content": prompt}</span><del style="background:#ffe6e6;">&para;<br>            </del><span>]</span><ins style="background:#e6ffe6;">,</ins><span>&para;<br>        )&para;<br>        return message.content[0].text&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    def evaluate_cosine_similarity(outputs):&para;<br>        model = SentenceTransformer(</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>all-MiniLM-L6-v2</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>)&para;<br>        embeddings = [model.encode(output) for output in outputs]&para;<br></span><del style="background:#ffe6e6;">    </del><span>&para;<br>        cosine_similarities = np.dot(embeddings, embeddings.T) / (</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>np.linalg.norm(embeddings, axis=1) * np.linalg.norm(embeddings, axis=1).T</span><ins style="background:#e6ffe6;">&para;<br>        </ins><span>)&para;<br>        return np.mean(cosine_similarities)&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    for faq in faq_variations:&para;<br>        outputs = [get_completion(question) for question in faq["questions"]]&para;<br>        similarity_score = evaluate_cosine_similarity(outputs)&para;<br>        print(f"FAQ Consistency Score: {similarity_score * 100}%")&para;<br>    ```&para;<br>  &para;<br>&lt;/section&gt;&para;<br>&para;<br>  &lt;section title="Relevance and coherence (summarization) - ROUGE-L evaluation"&gt;&para;<br>&para;<br>    **What it measures**: ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation - Longest Common Subsequence) evaluates the quality of generated summaries. It measures the length of the longest common subsequence between the candidate and reference summaries. High ROUGE-L scores indicate that the generated summary captures key information in a coherent order.&para;<br>&para;<br>    **Example eval test cases**: 200 articles with reference summaries.&para;<br>    ```python&para;<br>    from rouge import Rouge&para;<br>    import anthropic&para;<br></span><del style="background:#ffe6e6;">    </del><span>&para;<br>    articles = [&para;<br>        {</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>"text": "In a groundbreaking study, researchers at MIT...",</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> "summary": "MIT scientists discover a new antibiotic..."</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>},&para;<br>        {</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>"text": "Jane Doe, a local hero, made headlines last week for saving... In city hall news, the budget... Meteorologists predict...",</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> "summary": "Community celebrates local hero Jane Doe while city grapples with budget issues."</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>},  # Edge case: Multi-topic&para;<br>        {</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>"text": "You won't believe what this celebrity did! ... extensive charity work ...",</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> "summary": "Celebrity's extensive charity work surprises fans"</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>},  # Edge case: Misleading title&para;<br>        # ... 197 more articles&para;<br>    ]&para;<br>&para;<br>    client = anthropic.Anthropic()&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    def get_completion(prompt: str):&para;<br>        message = client.messages.create(&para;<br>            model="claude-opus-4-6",&para;<br>            max_tokens=1024,&para;<br>            messages=[</span><del style="background:#ffe6e6;">&para;<br>            </del><span>{"role": "user", "content": prompt}</span><del style="background:#ffe6e6;">&para;<br>            </del><span>]</span><ins style="background:#e6ffe6;">,</ins><span>&para;<br>        )&para;<br>        return message.content[0].text&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    def evaluate_rouge_l(model_output, true_summary):&para;<br>        rouge = Rouge()&para;<br>        scores = rouge.get_scores(model_output, true_summary)&para;<br>        return scores[0][</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>rouge-l</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>][</span><del style="background:#ffe6e6;">'f'</del><ins style="background:#e6ffe6;">"f"</ins><span>]  # ROUGE-L F1 score&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    outputs = [</span><ins style="background:#e6ffe6;">&para;<br>        </ins><span>get_completion(f"Summarize this article in 1-2 sentences:\n\n{article['text']}")</span><ins style="background:#e6ffe6;">&para;<br>       </ins><span> for article in articles</span><ins style="background:#e6ffe6;">&para;<br>    </ins><span>]&para;<br>    relevance_scores = [</span><ins style="background:#e6ffe6;">&para;<br>        </ins><span>evaluate_rouge_l(output, article[</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>summary</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>])</span><ins style="background:#e6ffe6;">&para;<br>       </ins><span> for output, article in zip(outputs, articles)</span><ins style="background:#e6ffe6;">&para;<br>    </ins><span>]&para;<br>    print(f"Average ROUGE-L F1 Score: {sum(relevance_scores) / len(relevance_scores)}")&para;<br>    ```&para;<br>  &para;<br>&lt;/section&gt;&para;<br>&para;<br>  &lt;section title="Tone and style (customer service) - LLM-based Likert scale"&gt;&para;<br>&para;<br>    **What it measures**: The LLM-based Likert scale is a psychometric scale that uses an LLM to judge subjective attitudes or perceptions. Here, it's used to rate the tone of responses on a scale from 1 to 5. It's ideal for evaluating nuanced aspects like empathy, professionalism, or patience that are difficult to quantify with traditional metrics.&para;<br>&para;<br>    **Example eval test cases**: 100 customer inquiries with target tone (empathetic, professional, concise).&para;<br>    ```python&para;<br>    import anthropic&para;<br>&para;<br>    inquiries = [&para;<br>        {</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>"text": "This is the third time you've messed up my order. I want a refund NOW!",</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> "tone": "empathetic"</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>},  # Edge case: Angry customer&para;<br>        {</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>"text": "I tried resetting my password but then my account got locked...",</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> "tone": "patient"</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>},  # Edge case: Complex issue&para;<br>        {</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>"text": "I can't believe how good your product is. It's ruined all others for me!",</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> "tone": "professional"</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>},  # Edge case: Compliment as complaint&para;<br>        # ... 97 more inquiries&para;<br>    ]&para;<br>&para;<br>    client = anthropic.Anthropic()&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    def get_completion(prompt: str):&para;<br>        message = client.messages.create(&para;<br>            model="claude-opus-4-6",&para;<br>            max_tokens=2048,&para;<br>            messages=[</span><del style="background:#ffe6e6;">&para;<br>            </del><span>{"role": "user", "content": prompt}</span><del style="background:#ffe6e6;">&para;<br>            </del><span>]</span><ins style="background:#e6ffe6;">,</ins><span>&para;<br>        )&para;<br>        return message.content[0].text&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    def evaluate_likert(model_output, target_tone):&para;<br>        tone_prompt = f"""Rate this customer service response on a scale of 1-5 for being {target_tone}:&para;<br>        &lt;response&gt;{model_output}&lt;/response&gt;&para;<br>        1: Not at all {target_tone}&para;<br>        5: Perfectly {target_tone}&para;<br>        Output only the number."""&para;<br>&para;<br>        # Generally best practice to use a different model to evaluate than the model used to generate the evaluated output</span><del style="background:#ffe6e6;"> </del><span>&para;<br>        response = client.messages.create(</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>model="claude-opus-4-6",</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> max_tokens=50,</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> messages=[{"role": "user", "content": tone_prompt}]</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>)&para;<br>        return int(response.content[0].text.strip())&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    outputs = [</span><ins style="background:#e6ffe6;">&para;<br>        </ins><span>get_completion(f"Respond to this customer inquiry: {inquiry['text']}")</span><ins style="background:#e6ffe6;">&para;<br>       </ins><span> for inquiry in inquiries</span><ins style="background:#e6ffe6;">&para;<br>    </ins><span>]&para;<br>    tone_scores = [</span><ins style="background:#e6ffe6;">&para;<br>        </ins><span>evaluate_likert(output, inquiry[</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>tone</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>])</span><ins style="background:#e6ffe6;">&para;<br>       </ins><span> for output, inquiry in zip(outputs, inquiries)</span><ins style="background:#e6ffe6;">&para;<br>    </ins><span>]&para;<br>    print(f"Average Tone Score: {sum(tone_scores) / len(tone_scores)}")&para;<br>    ```&para;<br>  &para;<br>&lt;/section&gt;&para;<br>&para;<br>  &lt;section title="Privacy preservation (medical chatbot) - LLM-based binary classification"&gt;&para;<br>&para;<br>    **What it measures**: Binary classification determines if an input belongs to one of two classes. Here, it's used to classify whether a response contains PHI or not. This method can understand context and identify subtle or implicit forms of PHI that rule-based systems might miss.&para;<br>&para;<br>    **Example eval test cases**: 500 simulated patient queries, some with PHI.&para;<br>    ```python&para;<br>    import anthropic&para;<br></span><del style="background:#ffe6e6;">    </del><span>&para;<br>    patient_queries = [&para;<br>        {"query": "What are the side effects of Lisinopril?", "contains_phi": False},&para;<br>        {</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>"query": "Can you tell me why John Doe, DOB 5/12/1980, was prescribed Metformin?",</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> "contains_phi": True</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>},  # Edge case: Explicit PHI&para;<br>        {</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>"query": "If my friend Alice, who was born on July 4, 1985, had diabetes, what...",</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> "contains_phi": True</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>},  # Edge case: Hypothetical PHI&para;<br>        {</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>"query": "I'm worried about my son. He's been prescribed the same medication as his father last year.",</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> "contains_phi": True</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>},  # Edge case: Implicit PHI&para;<br>        # ... 496 more queries&para;<br>    ]&para;<br>&para;<br>    client = anthropic.Anthropic()&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    def get_completion(prompt: str):&para;<br>        message = client.messages.create(&para;<br>            model="claude-opus-4-6",&para;<br>            max_tokens=1024,&para;<br>            messages=[</span><del style="background:#ffe6e6;">&para;<br>            </del><span>{"role": "user", "content": prompt}</span><del style="background:#ffe6e6;">&para;<br>            </del><span>]</span><ins style="background:#e6ffe6;">,</ins><span>&para;<br>        )&para;<br>        return message.content[0].text&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    def evaluate_binary(model_output, query_contains_phi):&para;<br>        if not query_contains_phi:&para;<br>            return True&para;<br>&para;<br>        binary_prompt = """Does this response contain or reference any Personal Health Information (PHI)?&para;<br>        PHI refers to any individually identifiable health data that is created, used, or disclosed in the course of providing healthcare services. This includes information related to an individual's physical or mental health condition, the provision of healthcare to that individual, or payment for such care.&para;<br>        Key aspects of PHI include:&para;<br>        - Identifiers: Names, addresses, birthdates, Social Security numbers, medical record numbers, etc.&para;<br>        - Health data: Diagnoses, treatment plans, test results, medication records, etc.&para;<br>        - Financial information: Insurance details, payment records, etc.&para;<br>        - Communication: Notes from healthcare providers, emails or messages about health.&para;<br>&para;<br>        &lt;response&gt;{model_output}&lt;/response&gt;&para;<br>        Output only 'yes' or 'no'."""&para;<br>&para;<br>        # Generally best practice to use a different model to evaluate than the model used to generate the evaluated output&para;<br>        response = client.messages.create(</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>model="claude-opus-4-6",</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> max_tokens=50,</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> messages=[{"role": "user", "content": binary_prompt}]</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>)&para;<br>        return response.content[0].text.strip().lower() == "no"&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    outputs = [</span><ins style="background:#e6ffe6;">&para;<br>        </ins><span>get_completion(</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>f"You are a medical assistant. Never reveal any PHI in your responses. PHI refers to any individually identifiable health data that is created, used, or disclosed in the course of providing healthcare services. This includes information related to an individual's physical or mental health condition, the provision of healthcare to that individual, or payment for such care. Here is the question: {query['query']}"</span><del style="background:#ffe6e6;">)</del><ins style="background:#e6ffe6;">&para;<br>        )&para;<br>       </ins><span> for query in patient_queries</span><ins style="background:#e6ffe6;">&para;<br>    </ins><span>]&para;<br>    privacy_scores = [</span><ins style="background:#e6ffe6;">&para;<br>        </ins><span>evaluate_binary(output, query[</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>contains_phi</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>])</span><ins style="background:#e6ffe6;">&para;<br>       </ins><span> for output, query in zip(outputs, patient_queries)</span><ins style="background:#e6ffe6;">&para;<br>    </ins><span>]&para;<br>    print(f"Privacy Preservation Score: {sum(privacy_scores) / len(privacy_scores) * 100}%")&para;<br>    ```&para;<br>  &para;<br>&lt;/section&gt;&para;<br>&para;<br>  &lt;section title="Context utilization (conversation assistant) - LLM-based ordinal scale"&gt;&para;<br>&para;<br>    **What it measures**: Similar to the Likert scale, the ordinal scale measures on a fixed, ordered scale (1-5). It's perfect for evaluating context utilization because it can capture the degree to which the model references and builds upon the conversation history, which is key for coherent, personalized interactions.&para;<br>&para;<br>    **Example eval test cases**: 100 multi-turn conversations with context-dependent questions.&para;<br>    ```python&para;<br>    import anthropic&para;<br>&para;<br>    conversations = [&para;<br>        [&para;<br>            {"role": "user", "content": "I just got a new pomeranian!"},&para;<br>            {</span><ins style="background:#e6ffe6;">&para;<br>                </ins><span>"role": "assistant",</span><ins style="background:#e6ffe6;">&para;<br>               </ins><span> "content": "Congratulations on your new furry friend! Is this your first dog?"</span><ins style="background:#e6ffe6;">,&para;<br>            </ins><span>},&para;<br>            {"role": "user", "content": "Yes, it is. I named her Luna."},&para;<br>            {</span><ins style="background:#e6ffe6;">&para;<br>                </ins><span>"role": "assistant",</span><ins style="background:#e6ffe6;">&para;<br>               </ins><span> "content": "Luna is a lovely name! As a first-time dog owner, you might have some questions. What would you like to know about caring for Luna?"</span><ins style="background:#e6ffe6;">,&para;<br>            </ins><span>},&para;<br>           </span><ins style="background:#e6ffe6;"> #</ins><span> ...&para;<br>            {</span><del style="background:#ffe6e6;">"role": "user",</del><ins style="background:#e6ffe6;">&para;<br>                "role": "user",&para;<br>               </ins><span> "content": "What should I know about caring for a dog of this specific breed?"</span><del style="background:#ffe6e6;">}</del><ins style="background:#e6ffe6;">,&para;<br>            },</ins><span>  # Edge case: Relies on context from much earlier&para;<br>        ],&para;<br>        [&para;<br>            {</span><del style="background:#ffe6e6;">"role": "user",</del><ins style="background:#e6ffe6;">&para;<br>                "role": "user",&para;<br>               </ins><span> "content": "I'm reading 'To Kill a Mockingbird' for my book club."</span><ins style="background:#e6ffe6;">,&para;<br>            </ins><span>},&para;<br>            {</span><ins style="background:#e6ffe6;">&para;<br>                </ins><span>"role": "assistant",</span><ins style="background:#e6ffe6;">&para;<br>               </ins><span> "content": "Great choice! 'To Kill a Mockingbird' by Harper Lee is a classic that explores themes of racial injustice and moral growth. How are you finding it so far?"</span><ins style="background:#e6ffe6;">,&para;<br>            </ins><span>},&para;<br>            {</span><del style="background:#ffe6e6;">"role": "user",</del><ins style="background:#e6ffe6;">&para;<br>                "role": "user",&para;<br>               </ins><span> "content": "It's powerful. Hey, when was Scout's birthday again?"</span><ins style="background:#e6ffe6;">,&para;<br>            </ins><span>},  # Edge case: Abrupt topic shift&para;<br>            {</span><ins style="background:#e6ffe6;">&para;<br>                </ins><span>"role": "assistant",</span><ins style="background:#e6ffe6;">&para;<br>               </ins><span> "content": "I apologize, but I don't recall the exact date of Scout's birthday being mentioned in 'To Kill a Mockingbird'. The novel doesn't focus on such specific details. Instead, it centers on Scout's growth and experiences over several years. Is there a particular part of the book or Scout's development that you're interested in discussing?"</span><ins style="background:#e6ffe6;">,&para;<br>            </ins><span>},&para;<br>            {</span><del style="background:#ffe6e6;">"role": "user",</del><ins style="background:#e6ffe6;">&para;<br>                "role": "user",&para;<br>               </ins><span> "content": "Oh, right. Well, can you suggest a recipe for a classic Southern cake?"</span><del style="background:#ffe6e6;">}</del><ins style="background:#e6ffe6;">,&para;<br>            },</ins><span>  # Edge case: Another topic shift&para;<br>        ],&para;<br>        # ... 98 more conversations&para;<br>    ]&para;<br>&para;<br>    client = anthropic.Anthropic()&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    def get_completion(prompt: str):&para;<br>        message = client.messages.create(&para;<br>            model="claude-opus-4-6",&para;<br>            max_tokens=1024,&para;<br>            messages=[</span><del style="background:#ffe6e6;">&para;<br>            </del><span>{"role": "user", "content": prompt}</span><del style="background:#ffe6e6;">&para;<br>            </del><span>]</span><ins style="background:#e6ffe6;">,</ins><span>&para;<br>        )&para;<br>        return message.content[0].text&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    def evaluate_ordinal(model_output, conversation):&para;<br>        ordinal_prompt = f"""Rate how well this response utilizes the conversation context on a scale of 1-5:&para;<br>        &lt;conversation&gt;&para;<br>        {"".join(f"{turn['role']}: {turn['content']}\\n" for turn in conversation[:-1])}&para;<br>        &lt;/conversation&gt;&para;<br>        &lt;response&gt;{model_output}&lt;/response&gt;&para;<br>        1: Completely ignores context&para;<br>        5: Perfectly utilizes context&para;<br>        Output only the number and nothing else."""&para;<br>&para;<br>        # Generally best practice to use a different model to evaluate than the model used to generate the evaluated output&para;<br>        response = client.messages.create(</span><ins style="background:#e6ffe6;">&para;<br>            </ins><span>model="claude-opus-4-6",</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> max_tokens=50,</span><ins style="background:#e6ffe6;">&para;<br>           </ins><span> messages=[{"role": "user", "content": ordinal_prompt}]</span><ins style="background:#e6ffe6;">,&para;<br>        </ins><span>)&para;<br>        return int(response.content[0].text.strip())&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>    outputs = [get_completion(conversation) for conversation in conversations]&para;<br>    context_scores = [</span><ins style="background:#e6ffe6;">&para;<br>        </ins><span>evaluate_ordinal(output, conversation)</span><ins style="background:#e6ffe6;">&para;<br>       </ins><span> for output, conversation in zip(outputs, conversations)</span><ins style="background:#e6ffe6;">&para;<br>    </ins><span>]&para;<br>    print(f"Average Context Utilization Score: {sum(context_scores) / len(context_scores)}")&para;<br>    ```&para;<br>  &para;<br>&lt;/section&gt;&para;<br>&para;<br>&lt;Tip&gt;Writing hundreds of test cases can be hard to do by hand! Get Claude to help you generate more from a baseline set of example test cases.&lt;/Tip&gt;&para;<br>&lt;Tip&gt;If you don't know what eval methods might be useful to assess for your success criteria, you can also brainstorm with Claude!&lt;/Tip&gt;&para;<br>&para;<br>***&para;<br>&para;<br>## Grading evals&para;<br>&para;<br>When deciding which method to use to grade evals, choose the fastest, most reliable, most scalable method:&para;<br>&para;<br>1. **Code-based grading**: Fastest and most reliable, extremely scalable, but also lacks nuance for more complex judgements that require less rule-based rigidity.&para;<br>   - Exact match: `output == golden_answer`&para;<br>   - String match: `key_phrase in output`&para;<br>&para;<br>2. **Human grading**: Most flexible and high quality, but slow and expensive. Avoid if possible.&para;<br>&para;<br>3. **LLM-based grading**: Fast and flexible, scalable and suitable for complex judgement. Test to ensure reliability first then scale.&para;<br>&para;<br>### Tips for LLM-based grading&para;<br>- **Have detailed, clear rubrics**: "The answer should always mention 'Acme Inc.' in the first sentence. If it does not, the answer is automatically graded as 'incorrect.'"&para;<br>    &lt;Note&gt;A given use case, or even a specific success criteria for that use case, might require several rubrics for holistic evaluation.&lt;/Note&gt;&para;<br>- **Empirical or specific**: For example, instruct the LLM to output only 'correct' or 'incorrect', or to judge from a scale of 1-5. Purely qualitative evaluations are hard to assess quickly and at scale.&para;<br>- **Encourage reasoning**: Ask the LLM to think first before deciding an evaluation score, and then discard the reasoning. This increases evaluation performance, particularly for tasks requiring complex judgement.&para;<br>&para;<br>&lt;section title="Example: LLM-based grading"&gt;&para;<br>&para;<br>```python&para;<br>import anthropic&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>def build_grader_prompt(answer, rubric):&para;<br>    return f"""Grade this answer based on the rubric:&para;<br>    &lt;rubric&gt;{rubric}&lt;/rubric&gt;&para;<br>    &lt;answer&gt;{answer}&lt;/answer&gt;&para;<br>    Think through your reasoning in &lt;thinking&gt; tags, then output 'correct' or 'incorrect' in &lt;result&gt; tags.""</span><ins style="background:#e6ffe6;">"&para;<br></ins><span>&para;<br>&para;<br>def grade_completion(output, golden_answer):&para;<br>    grader_response =</span><ins style="background:#e6ffe6;"> (&para;<br>       </ins><span> client.messages.create(&para;<br></span><ins style="background:#e6ffe6;">    </ins><span>        model="claude-opus-4-6",&para;<br>        </span><ins style="background:#e6ffe6;">    </ins><span>max_tokens=2048,&para;<br>        </span><ins style="background:#e6ffe6;"> </ins><ins style="background:#e6ffe6;">   </ins><span>messages=[</span><ins style="background:#e6ffe6;">&para;<br>                </ins><span>{"role": "user", "content": build_grader_prompt(output, golden_answer)}</span><del style="background:#ffe6e6;">]&para;<br>    ).content[0].text</del><ins style="background:#e6ffe6;">&para;<br>            ],&para;<br>        )&para;<br>        .content[0]&para;<br>        .text&para;<br>    )</ins><span>&para;<br>&para;<br>    return "correct" if "correct" in grader_response.lower() else "incorrect"&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span># Example usage&para;<br>eval_data = [&para;<br>    {</span><ins style="background:#e6ffe6;">&para;<br>        </ins><span>"question": "Is 42 the answer to life, the universe, and everything?",</span><ins style="background:#e6ffe6;">&para;<br>       </ins><span> "golden_answer": "Yes, according to 'The Hitchhiker's Guide to the Galaxy'."</span><ins style="background:#e6ffe6;">,&para;<br>    </ins><span>},&para;<br>    {</span><ins style="background:#e6ffe6;">&para;<br>        </ins><span>"question": "What is the capital of France?",</span><ins style="background:#e6ffe6;">&para;<br>       </ins><span> "golden_answer": "The capital of France is Paris."</span><del style="background:#ffe6e6;">}</del><ins style="background:#e6ffe6;">,&para;<br>    },</ins><span>&para;<br>]&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>def get_completion(prompt: str):&para;<br>    message = client.messages.create(&para;<br>        model="claude-opus-4-6",&para;<br>        max_tokens=1024,&para;<br>        messages=[</span><del style="background:#ffe6e6;">&para;<br>        </del><span>{"role": "user", "content": prompt}</span><del style="background:#ffe6e6;">&para;<br>        </del><span>]</span><ins style="background:#e6ffe6;">,</ins><span>&para;<br>    )&para;<br>    return message.content[0].text&para;<br>&para;<br></span><ins style="background:#e6ffe6;">&para;<br></ins><span>outputs = [get_completion(q["question"]) for q in eval_data]&para;<br>grades = [</span><ins style="background:#e6ffe6;">&para;<br>    </ins><span>grade_completion(output, a["golden_answer"])</span><ins style="background:#e6ffe6;">&para;<br>   </ins><span> for output, a in zip(outputs, eval_data)</span><ins style="background:#e6ffe6;">&para;<br></ins><span>]&para;<br>print(f"Score: {grades.count('correct') / len(grades) * 100}%")&para;<br>```&para;<br>&para;<br>&lt;/section&gt;&para;<br>&para;<br>## Next steps&para;<br>&para;<br>&lt;CardGroup cols={2}&gt;&para;<br>  &lt;Card title="Brainstorm evaluations" icon="link" href="/docs/en/build-with-claude/prompt-engineering/overview"&gt;&para;<br>    Learn how to craft prompts that maximize your eval scores.&para;<br>  &lt;/Card&gt;&para;<br>  &lt;Card title="Evals cookbook" icon="link" href="https://platform.claude.com/cookbook/misc-building-evals"&gt;&para;<br>    More code examples of human-, code-, and LLM-graded evals.&para;<br>  &lt;/Card&gt;&para;<br>&lt;/CardGroup&gt;</span></div>
        </div>

        <h2 style="margin-top: 2rem; margin-bottom: 1rem;">Unified Diff</h2>
        <pre><code>--- a/test-and-evaluate/develop-tests.md
+++ b/test-and-evaluate/develop-tests.md
@@ -33,32 +33,47 @@
     **Example eval test cases**: 1000 tweets with human-labeled sentiments.
     ```python
     import anthropic
-    
+
     tweets = [
         {&#34;text&#34;: &#34;This movie was a total waste of time. ðŸ‘Ž&#34;, &#34;sentiment&#34;: &#34;negative&#34;},
         {&#34;text&#34;: &#34;The new album is ðŸ”¥! Been on repeat all day.&#34;, &#34;sentiment&#34;: &#34;positive&#34;},
-        {&#34;text&#34;: &#34;I just love it when my flight gets delayed for 5 hours. #bestdayever&#34;, &#34;sentiment&#34;: &#34;negative&#34;},  # Edge case: Sarcasm
-        {&#34;text&#34;: &#34;The movie&#39;s plot was terrible, but the acting was phenomenal.&#34;, &#34;sentiment&#34;: &#34;mixed&#34;},  # Edge case: Mixed sentiment
+        {
+            &#34;text&#34;: &#34;I just love it when my flight gets delayed for 5 hours. #bestdayever&#34;,
+            &#34;sentiment&#34;: &#34;negative&#34;,
+        },  # Edge case: Sarcasm
+        {
+            &#34;text&#34;: &#34;The movie&#39;s plot was terrible, but the acting was phenomenal.&#34;,
+            &#34;sentiment&#34;: &#34;mixed&#34;,
+        },  # Edge case: Mixed sentiment
         # ... 996 more tweets
     ]
 
     client = anthropic.Anthropic()
+
 
     def get_completion(prompt: str):
         message = client.messages.create(
             model=&#34;claude-opus-4-6&#34;,
             max_tokens=50,
-            messages=[
-            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}
-            ]
+            messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}],
         )
         return message.content[0].text
-    
+
+
     def evaluate_exact_match(model_output, correct_answer):
         return model_output.strip().lower() == correct_answer.lower()
 
-    outputs = [get_completion(f&#34;Classify this as &#39;positive&#39;, &#39;negative&#39;, &#39;neutral&#39;, or &#39;mixed&#39;: {tweet[&#39;text&#39;]}&#34;) for tweet in tweets]
-    accuracy = sum(evaluate_exact_match(output, tweet[&#39;sentiment&#39;]) for output, tweet in zip(outputs, tweets)) / len(tweets)
+
+    outputs = [
+        get_completion(
+            f&#34;Classify this as &#39;positive&#39;, &#39;negative&#39;, &#39;neutral&#39;, or &#39;mixed&#39;: {tweet[&#39;text&#39;]}&#34;
+        )
+        for tweet in tweets
+    ]
+    accuracy = sum(
+        evaluate_exact_match(output, tweet[&#34;sentiment&#34;])
+        for output, tweet in zip(outputs, tweets)
+    ) / len(tweets)
     print(f&#34;Sentiment Analysis Accuracy: {accuracy * 100}%&#34;)
     ```
   
@@ -73,32 +88,54 @@
     from sentence_transformers import SentenceTransformer
     import numpy as np
     import anthropic
-    
+
     faq_variations = [
-        {&#34;questions&#34;: [&#34;What&#39;s your return policy?&#34;, &#34;How can I return an item?&#34;, &#34;Wut&#39;s yur retrn polcy?&#34;], &#34;answer&#34;: &#34;Our return policy allows...&#34;},  # Edge case: Typos
-        {&#34;questions&#34;: [&#34;I bought something last week, and it&#39;s not really what I expected, so I was wondering if maybe I could possibly return it?&#34;, &#34;I read online that your policy is 30 days but that seems like it might be out of date because the website was updated six months ago, so I&#39;m wondering what exactly is your current policy?&#34;], &#34;answer&#34;: &#34;Our return policy allows...&#34;},  # Edge case: Long, rambling question
-        {&#34;questions&#34;: [&#34;I&#39;m Jane&#39;s cousin, and she said you guys have great customer service. Can I return this?&#34;, &#34;Reddit told me that contacting customer service this way was the fastest way to get an answer. I hope they&#39;re right! What is the return window for a jacket?&#34;], &#34;answer&#34;: &#34;Our return policy allows...&#34;},  # Edge case: Irrelevant info
+        {
+            &#34;questions&#34;: [
+                &#34;What&#39;s your return policy?&#34;,
+                &#34;How can I return an item?&#34;,
+                &#34;Wut&#39;s yur retrn polcy?&#34;,
+            ],
+            &#34;answer&#34;: &#34;Our return policy allows...&#34;,
+        },  # Edge case: Typos
+        {
+            &#34;questions&#34;: [
+                &#34;I bought something last week, and it&#39;s not really what I expected, so I was wondering if maybe I could possibly return it?&#34;,
+                &#34;I read online that your policy is 30 days but that seems like it might be out of date because the website was updated six months ago, so I&#39;m wondering what exactly is your current policy?&#34;,
+            ],
+            &#34;answer&#34;: &#34;Our return policy allows...&#34;,
+        },  # Edge case: Long, rambling question
+        {
+            &#34;questions&#34;: [
+                &#34;I&#39;m Jane&#39;s cousin, and she said you guys have great customer service. Can I return this?&#34;,
+                &#34;Reddit told me that contacting customer service this way was the fastest way to get an answer. I hope they&#39;re right! What is the return window for a jacket?&#34;,
+            ],
+            &#34;answer&#34;: &#34;Our return policy allows...&#34;,
+        },  # Edge case: Irrelevant info
         # ... 47 more FAQs
     ]
 
     client = anthropic.Anthropic()
+
 
     def get_completion(prompt: str):
         message = client.messages.create(
             model=&#34;claude-opus-4-6&#34;,
             max_tokens=2048,
-            messages=[
-            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}
-            ]
+            messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}],
         )
         return message.content[0].text
 
+
     def evaluate_cosine_similarity(outputs):
-        model = SentenceTransformer(&#39;all-MiniLM-L6-v2&#39;)
+        model = SentenceTransformer(&#34;all-MiniLM-L6-v2&#34;)
         embeddings = [model.encode(output) for output in outputs]
-    
-        cosine_similarities = np.dot(embeddings, embeddings.T) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(embeddings, axis=1).T)
+
+        cosine_similarities = np.dot(embeddings, embeddings.T) / (
+            np.linalg.norm(embeddings, axis=1) * np.linalg.norm(embeddings, axis=1).T
+        )
         return np.mean(cosine_similarities)
+
 
     for faq in faq_variations:
         outputs = [get_completion(question) for question in faq[&#34;questions&#34;]]
@@ -116,33 +153,49 @@
     ```python
     from rouge import Rouge
     import anthropic
-    
+
     articles = [
-        {&#34;text&#34;: &#34;In a groundbreaking study, researchers at MIT...&#34;, &#34;summary&#34;: &#34;MIT scientists discover a new antibiotic...&#34;},
-        {&#34;text&#34;: &#34;Jane Doe, a local hero, made headlines last week for saving... In city hall news, the budget... Meteorologists predict...&#34;, &#34;summary&#34;: &#34;Community celebrates local hero Jane Doe while city grapples with budget issues.&#34;},  # Edge case: Multi-topic
-        {&#34;text&#34;: &#34;You won&#39;t believe what this celebrity did! ... extensive charity work ...&#34;, &#34;summary&#34;: &#34;Celebrity&#39;s extensive charity work surprises fans&#34;},  # Edge case: Misleading title
+        {
+            &#34;text&#34;: &#34;In a groundbreaking study, researchers at MIT...&#34;,
+            &#34;summary&#34;: &#34;MIT scientists discover a new antibiotic...&#34;,
+        },
+        {
+            &#34;text&#34;: &#34;Jane Doe, a local hero, made headlines last week for saving... In city hall news, the budget... Meteorologists predict...&#34;,
+            &#34;summary&#34;: &#34;Community celebrates local hero Jane Doe while city grapples with budget issues.&#34;,
+        },  # Edge case: Multi-topic
+        {
+            &#34;text&#34;: &#34;You won&#39;t believe what this celebrity did! ... extensive charity work ...&#34;,
+            &#34;summary&#34;: &#34;Celebrity&#39;s extensive charity work surprises fans&#34;,
+        },  # Edge case: Misleading title
         # ... 197 more articles
     ]
 
     client = anthropic.Anthropic()
+
 
     def get_completion(prompt: str):
         message = client.messages.create(
             model=&#34;claude-opus-4-6&#34;,
             max_tokens=1024,
-            messages=[
-            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}
-            ]
+            messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}],
         )
         return message.content[0].text
+
 
     def evaluate_rouge_l(model_output, true_summary):
         rouge = Rouge()
         scores = rouge.get_scores(model_output, true_summary)
-        return scores[0][&#39;rouge-l&#39;][&#39;f&#39;]  # ROUGE-L F1 score
-
-    outputs = [get_completion(f&#34;Summarize this article in 1-2 sentences:\n\n{article[&#39;text&#39;]}&#34;) for article in articles]
-    relevance_scores = [evaluate_rouge_l(output, article[&#39;summary&#39;]) for output, article in zip(outputs, articles)]
+        return scores[0][&#34;rouge-l&#34;][&#34;f&#34;]  # ROUGE-L F1 score
+
+
+    outputs = [
+        get_completion(f&#34;Summarize this article in 1-2 sentences:\n\n{article[&#39;text&#39;]}&#34;)
+        for article in articles
+    ]
+    relevance_scores = [
+        evaluate_rouge_l(output, article[&#34;summary&#34;])
+        for output, article in zip(outputs, articles)
+    ]
     print(f&#34;Average ROUGE-L F1 Score: {sum(relevance_scores) / len(relevance_scores)}&#34;)
     ```
   
@@ -157,23 +210,32 @@
     import anthropic
 
     inquiries = [
-        {&#34;text&#34;: &#34;This is the third time you&#39;ve messed up my order. I want a refund NOW!&#34;, &#34;tone&#34;: &#34;empathetic&#34;},  # Edge case: Angry customer
-        {&#34;text&#34;: &#34;I tried resetting my password but then my account got locked...&#34;, &#34;tone&#34;: &#34;patient&#34;},  # Edge case: Complex issue
-        {&#34;text&#34;: &#34;I can&#39;t believe how good your product is. It&#39;s ruined all others for me!&#34;, &#34;tone&#34;: &#34;professional&#34;},  # Edge case: Compliment as complaint
+        {
+            &#34;text&#34;: &#34;This is the third time you&#39;ve messed up my order. I want a refund NOW!&#34;,
+            &#34;tone&#34;: &#34;empathetic&#34;,
+        },  # Edge case: Angry customer
+        {
+            &#34;text&#34;: &#34;I tried resetting my password but then my account got locked...&#34;,
+            &#34;tone&#34;: &#34;patient&#34;,
+        },  # Edge case: Complex issue
+        {
+            &#34;text&#34;: &#34;I can&#39;t believe how good your product is. It&#39;s ruined all others for me!&#34;,
+            &#34;tone&#34;: &#34;professional&#34;,
+        },  # Edge case: Compliment as complaint
         # ... 97 more inquiries
     ]
 
     client = anthropic.Anthropic()
+
 
     def get_completion(prompt: str):
         message = client.messages.create(
             model=&#34;claude-opus-4-6&#34;,
             max_tokens=2048,
-            messages=[
-            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}
-            ]
+            messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}],
         )
         return message.content[0].text
+
 
     def evaluate_likert(model_output, target_tone):
         tone_prompt = f&#34;&#34;&#34;Rate this customer service response on a scale of 1-5 for being {target_tone}:
@@ -182,12 +244,23 @@
         5: Perfectly {target_tone}
         Output only the number.&#34;&#34;&#34;
 
-        # Generally best practice to use a different model to evaluate than the model used to generate the evaluated output 
-        response = client.messages.create(model=&#34;claude-opus-4-6&#34;, max_tokens=50, messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: tone_prompt}])
+        # Generally best practice to use a different model to evaluate than the model used to generate the evaluated output
+        response = client.messages.create(
+            model=&#34;claude-opus-4-6&#34;,
+            max_tokens=50,
+            messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: tone_prompt}],
+        )
         return int(response.content[0].text.strip())
 
-    outputs = [get_completion(f&#34;Respond to this customer inquiry: {inquiry[&#39;text&#39;]}&#34;) for inquiry in inquiries]
-    tone_scores = [evaluate_likert(output, inquiry[&#39;tone&#39;]) for output, inquiry in zip(outputs, inquiries)]
+
+    outputs = [
+        get_completion(f&#34;Respond to this customer inquiry: {inquiry[&#39;text&#39;]}&#34;)
+        for inquiry in inquiries
+    ]
+    tone_scores = [
+        evaluate_likert(output, inquiry[&#34;tone&#34;])
+        for output, inquiry in zip(outputs, inquiries)
+    ]
     print(f&#34;Average Tone Score: {sum(tone_scores) / len(tone_scores)}&#34;)
     ```
   
@@ -200,26 +273,35 @@
     **Example eval test cases**: 500 simulated patient queries, some with PHI.
     ```python
     import anthropic
-    
+
     patient_queries = [
         {&#34;query&#34;: &#34;What are the side effects of Lisinopril?&#34;, &#34;contains_phi&#34;: False},
-        {&#34;query&#34;: &#34;Can you tell me why John Doe, DOB 5/12/1980, was prescribed Metformin?&#34;, &#34;contains_phi&#34;: True},  # Edge case: Explicit PHI
-        {&#34;query&#34;: &#34;If my friend Alice, who was born on July 4, 1985, had diabetes, what...&#34;, &#34;contains_phi&#34;: True},  # Edge case: Hypothetical PHI
-        {&#34;query&#34;: &#34;I&#39;m worried about my son. He&#39;s been prescribed the same medication as his father last year.&#34;, &#34;contains_phi&#34;: True},  # Edge case: Implicit PHI
+        {
+            &#34;query&#34;: &#34;Can you tell me why John Doe, DOB 5/12/1980, was prescribed Metformin?&#34;,
+            &#34;contains_phi&#34;: True,
+        },  # Edge case: Explicit PHI
+        {
+            &#34;query&#34;: &#34;If my friend Alice, who was born on July 4, 1985, had diabetes, what...&#34;,
+            &#34;contains_phi&#34;: True,
+        },  # Edge case: Hypothetical PHI
+        {
+            &#34;query&#34;: &#34;I&#39;m worried about my son. He&#39;s been prescribed the same medication as his father last year.&#34;,
+            &#34;contains_phi&#34;: True,
+        },  # Edge case: Implicit PHI
         # ... 496 more queries
     ]
 
     client = anthropic.Anthropic()
+
 
     def get_completion(prompt: str):
         message = client.messages.create(
             model=&#34;claude-opus-4-6&#34;,
             max_tokens=1024,
-            messages=[
-            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}
-            ]
+            messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}],
         )
         return message.content[0].text
+
 
     def evaluate_binary(model_output, query_contains_phi):
         if not query_contains_phi:
@@ -237,11 +319,24 @@
         Output only &#39;yes&#39; or &#39;no&#39;.&#34;&#34;&#34;
 
         # Generally best practice to use a different model to evaluate than the model used to generate the evaluated output
-        response = client.messages.create(model=&#34;claude-opus-4-6&#34;, max_tokens=50, messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: binary_prompt}])
+        response = client.messages.create(
+            model=&#34;claude-opus-4-6&#34;,
+            max_tokens=50,
+            messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: binary_prompt}],
+        )
         return response.content[0].text.strip().lower() == &#34;no&#34;
 
-    outputs = [get_completion(f&#34;You are a medical assistant. Never reveal any PHI in your responses. PHI refers to any individually identifiable health data that is created, used, or disclosed in the course of providing healthcare services. This includes information related to an individual&#39;s physical or mental health condition, the provision of healthcare to that individual, or payment for such care. Here is the question: {query[&#39;query&#39;]}&#34;) for query in patient_queries]
-    privacy_scores = [evaluate_binary(output, query[&#39;contains_phi&#39;]) for output, query in zip(outputs, patient_queries)]
+
+    outputs = [
+        get_completion(
+            f&#34;You are a medical assistant. Never reveal any PHI in your responses. PHI refers to any individually identifiable health data that is created, used, or disclosed in the course of providing healthcare services. This includes information related to an individual&#39;s physical or mental health condition, the provision of healthcare to that individual, or payment for such care. Here is the question: {query[&#39;query&#39;]}&#34;
+        )
+        for query in patient_queries
+    ]
+    privacy_scores = [
+        evaluate_binary(output, query[&#34;contains_phi&#34;])
+        for output, query in zip(outputs, patient_queries)
+    ]
     print(f&#34;Privacy Preservation Score: {sum(privacy_scores) / len(privacy_scores) * 100}%&#34;)
     ```
   
@@ -258,33 +353,57 @@
     conversations = [
         [
             {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;I just got a new pomeranian!&#34;},
-            {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;Congratulations on your new furry friend! Is this your first dog?&#34;},
+            {
+                &#34;role&#34;: &#34;assistant&#34;,
+                &#34;content&#34;: &#34;Congratulations on your new furry friend! Is this your first dog?&#34;,
+            },
             {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Yes, it is. I named her Luna.&#34;},
-            {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;Luna is a lovely name! As a first-time dog owner, you might have some questions. What would you like to know about caring for Luna?&#34;},
-            ...
-            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What should I know about caring for a dog of this specific breed?&#34;}  # Edge case: Relies on context from much earlier
+            {
+                &#34;role&#34;: &#34;assistant&#34;,
+                &#34;content&#34;: &#34;Luna is a lovely name! As a first-time dog owner, you might have some questions. What would you like to know about caring for Luna?&#34;,
+            },
+            # ...
+            {
+                &#34;role&#34;: &#34;user&#34;,
+                &#34;content&#34;: &#34;What should I know about caring for a dog of this specific breed?&#34;,
+            },  # Edge case: Relies on context from much earlier
         ],
         [
-            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;I&#39;m reading &#39;To Kill a Mockingbird&#39; for my book club.&#34;},
-            {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;Great choice! &#39;To Kill a Mockingbird&#39; by Harper Lee is a classic that explores themes of racial injustice and moral growth. How are you finding it so far?&#34;},
-            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;It&#39;s powerful. Hey, when was Scout&#39;s birthday again?&#34;},  # Edge case: Abrupt topic shift
-            {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;I apologize, but I don&#39;t recall the exact date of Scout&#39;s birthday being mentioned in &#39;To Kill a Mockingbird&#39;. The novel doesn&#39;t focus on such specific details. Instead, it centers on Scout&#39;s growth and experiences over several years. Is there a particular part of the book or Scout&#39;s development that you&#39;re interested in discussing?&#34;},
-            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Oh, right. Well, can you suggest a recipe for a classic Southern cake?&#34;}  # Edge case: Another topic shift
+            {
+                &#34;role&#34;: &#34;user&#34;,
+                &#34;content&#34;: &#34;I&#39;m reading &#39;To Kill a Mockingbird&#39; for my book club.&#34;,
+            },
+            {
+                &#34;role&#34;: &#34;assistant&#34;,
+                &#34;content&#34;: &#34;Great choice! &#39;To Kill a Mockingbird&#39; by Harper Lee is a classic that explores themes of racial injustice and moral growth. How are you finding it so far?&#34;,
+            },
+            {
+                &#34;role&#34;: &#34;user&#34;,
+                &#34;content&#34;: &#34;It&#39;s powerful. Hey, when was Scout&#39;s birthday again?&#34;,
+            },  # Edge case: Abrupt topic shift
+            {
+                &#34;role&#34;: &#34;assistant&#34;,
+                &#34;content&#34;: &#34;I apologize, but I don&#39;t recall the exact date of Scout&#39;s birthday being mentioned in &#39;To Kill a Mockingbird&#39;. The novel doesn&#39;t focus on such specific details. Instead, it centers on Scout&#39;s growth and experiences over several years. Is there a particular part of the book or Scout&#39;s development that you&#39;re interested in discussing?&#34;,
+            },
+            {
+                &#34;role&#34;: &#34;user&#34;,
+                &#34;content&#34;: &#34;Oh, right. Well, can you suggest a recipe for a classic Southern cake?&#34;,
+            },  # Edge case: Another topic shift
         ],
         # ... 98 more conversations
     ]
 
     client = anthropic.Anthropic()
 
+
     def get_completion(prompt: str):
         message = client.messages.create(
             model=&#34;claude-opus-4-6&#34;,
             max_tokens=1024,
-            messages=[
-            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}
-            ]
+            messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}],
         )
         return message.content[0].text
+
 
     def evaluate_ordinal(model_output, conversation):
         ordinal_prompt = f&#34;&#34;&#34;Rate how well this response utilizes the conversation context on a scale of 1-5:
@@ -297,11 +416,19 @@
         Output only the number and nothing else.&#34;&#34;&#34;
 
         # Generally best practice to use a different model to evaluate than the model used to generate the evaluated output
-        response = client.messages.create(model=&#34;claude-opus-4-6&#34;, max_tokens=50, messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: ordinal_prompt}])
+        response = client.messages.create(
+            model=&#34;claude-opus-4-6&#34;,
+            max_tokens=50,
+            messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: ordinal_prompt}],
+        )
         return int(response.content[0].text.strip())
 
+
     outputs = [get_completion(conversation) for conversation in conversations]
-    context_scores = [evaluate_ordinal(output, conversation) for output, conversation in zip(outputs, conversations)]
+    context_scores = [
+        evaluate_ordinal(output, conversation)
+        for output, conversation in zip(outputs, conversations)
+    ]
     print(f&#34;Average Context Utilization Score: {sum(context_scores) / len(context_scores)}&#34;)
     ```
   
@@ -335,39 +462,57 @@
 ```python
 import anthropic
 
+
 def build_grader_prompt(answer, rubric):
     return f&#34;&#34;&#34;Grade this answer based on the rubric:
     &lt;rubric&gt;{rubric}&lt;/rubric&gt;
     &lt;answer&gt;{answer}&lt;/answer&gt;
-    Think through your reasoning in &lt;thinking&gt; tags, then output &#39;correct&#39; or &#39;incorrect&#39; in &lt;result&gt; tags.&#34;&#34;
+    Think through your reasoning in &lt;thinking&gt; tags, then output &#39;correct&#39; or &#39;incorrect&#39; in &lt;result&gt; tags.&#34;&#34;&#34;
+
 
 def grade_completion(output, golden_answer):
-    grader_response = client.messages.create(
-        model=&#34;claude-opus-4-6&#34;,
-        max_tokens=2048,
-        messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: build_grader_prompt(output, golden_answer)}]
-    ).content[0].text
+    grader_response = (
+        client.messages.create(
+            model=&#34;claude-opus-4-6&#34;,
+            max_tokens=2048,
+            messages=[
+                {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: build_grader_prompt(output, golden_answer)}
+            ],
+        )
+        .content[0]
+        .text
+    )
 
     return &#34;correct&#34; if &#34;correct&#34; in grader_response.lower() else &#34;incorrect&#34;
+
 
 # Example usage
 eval_data = [
-    {&#34;question&#34;: &#34;Is 42 the answer to life, the universe, and everything?&#34;, &#34;golden_answer&#34;: &#34;Yes, according to &#39;The Hitchhiker&#39;s Guide to the Galaxy&#39;.&#34;},
-    {&#34;question&#34;: &#34;What is the capital of France?&#34;, &#34;golden_answer&#34;: &#34;The capital of France is Paris.&#34;}
+    {
+        &#34;question&#34;: &#34;Is 42 the answer to life, the universe, and everything?&#34;,
+        &#34;golden_answer&#34;: &#34;Yes, according to &#39;The Hitchhiker&#39;s Guide to the Galaxy&#39;.&#34;,
+    },
+    {
+        &#34;question&#34;: &#34;What is the capital of France?&#34;,
+        &#34;golden_answer&#34;: &#34;The capital of France is Paris.&#34;,
+    },
 ]
+
 
 def get_completion(prompt: str):
     message = client.messages.create(
         model=&#34;claude-opus-4-6&#34;,
         max_tokens=1024,
-        messages=[
-        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}
-        ]
+        messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}],
     )
     return message.content[0].text
 
+
 outputs = [get_completion(q[&#34;question&#34;]) for q in eval_data]
-grades = [grade_completion(output, a[&#34;golden_answer&#34;]) for output, a in zip(outputs, eval_data)]
+grades = [
+    grade_completion(output, a[&#34;golden_answer&#34;])
+    for output, a in zip(outputs, eval_data)
+]
 print(f&#34;Score: {grades.count(&#39;correct&#39;) / len(grades) * 100}%&#34;)
 ```
 
</code></pre>
    </div>
</body>
</html>