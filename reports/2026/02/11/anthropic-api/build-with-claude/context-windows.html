<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>build-with-claude/context-windows - Diff Report</title>
    <link rel="stylesheet" href="../../css/diff.css">
    <style>
        :root {
            --bg-color: #1a1a2e;
            --card-bg: #16213e;
            --text-color: #eee;
            --accent: #0f3460;
            --add-bg: #1a4d1a;
            --del-bg: #4d1a1a;
            --add-text: #4ade80;
            --del-text: #f87171;
        }
        @media (prefers-color-scheme: light) {
            :root {
                --bg-color: #f5f5f5;
                --card-bg: #fff;
                --text-color: #333;
                --accent: #e0e0e0;
                --add-bg: #d4edda;
                --del-bg: #f8d7da;
                --add-text: #155724;
                --del-text: #721c24;
            }
        }
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: var(--bg-color);
            color: var(--text-color);
            line-height: 1.6;
            padding: 2rem;
        }
        .container { max-width: 1200px; margin: 0 auto; }
        header { margin-bottom: 2rem; }
        h1 { font-size: 1.5rem; margin-bottom: 0.5rem; }
        .meta { color: #888; font-size: 0.9rem; }
        .summary {
            background: var(--card-bg);
            padding: 1rem;
            border-radius: 8px;
            margin-bottom: 2rem;
            display: flex;
            gap: 2rem;
        }
        .stat { display: flex; align-items: center; gap: 0.5rem; }
        .stat.added { color: var(--add-text); }
        .stat.removed { color: var(--del-text); }
        .analysis {
            background: var(--card-bg);
            padding: 1.5rem;
            border-radius: 8px;
            margin-bottom: 2rem;
            border-left: 4px solid #8b5cf6;
        }
        .analysis-header {
            font-weight: 600;
            margin-bottom: 0.75rem;
            color: #8b5cf6;
        }
        .analysis-content {
            white-space: pre-wrap;
            font-size: 0.95rem;
            line-height: 1.7;
        }
        .diff-container {
            background: var(--card-bg);
            border-radius: 8px;
            overflow: hidden;
        }
        .diff-header {
            background: var(--accent);
            padding: 0.75rem 1rem;
            font-weight: 600;
        }
        .diff-content {
            padding: 1rem;
            overflow-x: auto;
        }
        .diff-content ins {
            background: var(--add-bg);
            color: var(--add-text);
            text-decoration: none;
            padding: 0.1em 0.2em;
            border-radius: 2px;
        }
        .diff-content del {
            background: var(--del-bg);
            color: var(--del-text);
            text-decoration: line-through;
            padding: 0.1em 0.2em;
            border-radius: 2px;
        }
        pre {
            background: var(--accent);
            padding: 1rem;
            border-radius: 8px;
            overflow-x: auto;
            font-size: 0.85rem;
            margin-top: 2rem;
        }
        a { color: #60a5fa; }
        .back-link { margin-bottom: 1rem; display: inline-block; }
    </style>
</head>
<body>
    <div class="container">
        <a href="../../index.html" class="back-link">&larr; Back to daily report</a>

        <header>
            <h1>build-with-claude/context-windows.md</h1>
            <p class="meta">Changed on 2026-02-11 10:56:44 EST</p>
        </header>

        <div class="summary">
            <div class="stat added">
                <span>+9</span> lines added
            </div>
            <div class="stat removed">
                <span>-11</span> lines removed
            </div>
        </div>

        

        <div class="diff-container">
            <div class="diff-header">Visual Diff</div>
            <div class="diff-content"><span># Context windows&para;<br>&para;<br>---&para;<br>&para;<br>As conversations grow, you'll eventually approach context window limits. This guide explains how context windows work and introduces strategies for managing them effectively.&para;<br>&para;<br>For long-running conversations and agentic workflows, [server-side compaction](/docs/en/build-with-claude/compaction) is the primary strategy for context management. For more specialized needs, [context editing](/docs/en/build-with-claude/context-editing) offers additional strategies like tool result clearing and thinking block clearing.&para;<br>&para;<br>## Understanding the context window&para;<br>&para;<br>The "context window" refers to all the text a language model can reference when generating a response, including the response itself. This is different from the large corpus of data the language model was trained on, and instead represents a "working memory" for the model. A larger context window allows the model to handle more complex and lengthy prompts. A smaller context window may limit the model's ability to maintain coherence over extended conversations.&para;<br>&para;<br>The diagram below illustrates the standard context window behavior for API requests&lt;sup&gt;1&lt;/sup&gt;:&para;<br>&para;<br>![Context window diagram](/docs/images/context-window.svg)&para;<br>&para;<br>_&lt;sup&gt;1&lt;/sup&gt;For chat interfaces, such as for [claude.ai](https://claude.ai/), context windows can also be set up on a rolling "first in, first out" system._&para;<br>&para;<br>* **Progressive token accumulation:** As the conversation advances through turns, each user message and assistant response accumulates within the context window. Previous turns are preserved completely.&para;<br>* **Linear growth pattern:** The context usage grows linearly with each turn, with previous turns preserved completely.&para;<br>* **200K token capacity:** The total available context window (200,000 tokens) represents the maximum capacity for storing conversation history and generating new output from Claude.&para;<br>* **Input-output flow:** Each turn consists of:&para;<br>  - **Input phase:** Contains all previous conversation history plus the current user message&para;<br>  - **Output phase:** Generates a text response that becomes part of a future input&para;<br>&para;<br>## The context window with extended thinking&para;<br>&para;<br>When using [extended thinking](/docs/en/build-with-claude/extended-thinking), all input and output tokens, including the tokens used for thinking, count toward the context window limit, with a few nuances in multi-turn situations.&para;<br>&para;<br>The thinking budget tokens are a subset of your `max_tokens` parameter, are billed as output tokens, and count towards rate limits. With [adaptive thinking](/docs/en/build-with-claude/adaptive-thinking), Claude dynamically decides its thinking allocation, so actual thinking token usage may vary per request.&para;<br>&para;<br>However, previous thinking blocks are automatically stripped from the context window calculation by the Claude API and are not part of the conversation history that the model "sees" for subsequent turns, preserving token capacity for actual conversation content.&para;<br>&para;<br>The diagram below demonstrates the specialized token management when extended thinking is enabled:&para;<br>&para;<br>![Context window diagram with extended thinking](/docs/images/context-window-thinking.svg)&para;<br>&para;<br>* **Stripping extended thinking:** Extended thinking blocks (shown in dark gray) are generated during each turn's output phase, **but are not carried forward as input tokens for subsequent turns**. You do not need to strip the thinking blocks yourself. The Claude API automatically does this for you if you pass them back.&para;<br>* **Technical implementation details:**&para;<br>  - The API automatically excludes thinking blocks from previous turns when you pass them back as part of the conversation history.&para;<br>  - Extended thinking tokens are billed as output tokens only once, during their generation.&para;<br>  - The effective context window calculation becomes: `context_window = (input_tokens - previous_thinking_tokens) + current_turn_tokens`.&para;<br>  - Thinking tokens include both `thinking` blocks and `redacted_thinking` blocks.&para;<br>&para;<br>This architecture is token efficient and allows for extensive reasoning without token waste, as thinking blocks can be substantial in length.&para;<br>&para;<br>&lt;Note&gt;&para;<br>You can read more about the context window and extended thinking in the [extended thinking guide](/docs/en/build-with-claude/extended-thinking).&para;<br>&lt;/Note&gt;&para;<br>&para;<br>## The context window with extended thinking and tool use&para;<br>&para;<br>The diagram below illustrates the context window token management when combining extended thinking with tool use:&para;<br>&para;<br>![Context window diagram with extended thinking and tool use](/docs/images/context-window-thinking-tools.svg)&para;<br>&para;<br>&lt;Steps&gt;&para;<br>  &lt;Step title="First turn architecture"&gt;&para;<br>    - **Input components:** Tools configuration and user message&para;<br>    - **Output components:** Extended thinking + text response + tool use request&para;<br>    - **Token calculation:** All input and output components count toward the context window, and all output components are billed as output tokens.&para;<br>  &lt;/Step&gt;&para;<br>  &lt;Step title="Tool result handling (turn 2)"&gt;&para;<br>    - **Input components:** Every block in the first turn as well as the `tool_result`. The extended thinking block **must** be returned with the corresponding tool results. This is the only case wherein you **have to** return thinking blocks.&para;<br>    - **Output components:** After tool results have been passed back to Claude, Claude will respond with only text (no additional extended thinking until the next `user` message).&para;<br>    - **Token calculation:** All input and output components count toward the context window, and all output components are billed as output tokens.&para;<br>  &lt;/Step&gt;&para;<br>  &lt;Step title="Third Step"&gt;&para;<br>    - **Input components:** All inputs and the output from the previous turn is carried forward with the exception of the thinking block, which can be dropped now that Claude has completed the entire tool use cycle. The API will automatically strip the thinking block for you if you pass it back, or you can feel free to strip it yourself at this stage. This is also where you would add the next `User` turn.&para;<br>    - **Output components:** Since there is a new `User` turn outside of the tool use cycle, Claude will generate a new extended thinking block and continue from there.&para;<br>    - **Token calculation:** Previous thinking tokens are automatically stripped from context window calculations. All other previous blocks still count as part of the token window, and the thinking block in the current `Assistant` turn counts as part of the context window.</span><del style="background:#ffe6e6;"> </del><span>&para;<br>  &lt;/Step&gt;&para;<br>&lt;/Steps&gt;&para;<br>&para;<br>* **Considerations for tool use with extended thinking:**&para;<br>  - When posting tool results, the entire unmodified thinking block that accompanies that specific tool request (including signature/redacted portions) must be included.&para;<br>  - The effective context window calculation for extended thinking with tool use becomes: `context_window = input_tokens + current_turn_tokens`.&para;<br>  - The system uses cryptographic signatures to verify thinking block authenticity. Failing to preserve thinking blocks during tool use can break Claude's reasoning continuity. Thus, if you modify thinking blocks, the API will return an error.&para;<br>&para;<br>&lt;Note&gt;&para;<br>Claude 4 models support [interleaved thinking](/docs/en/build-with-claude/extended-thinking#interleaved-thinking), which enables Claude to think between tool calls and make more sophisticated reasoning after receiving tool results.&para;<br>&para;<br>Claude Sonnet 3.7 does not support interleaved thinking, so there is no interleaving of extended thinking and tool calls without a non-`tool_result` user turn in between.&para;<br>&para;<br>For more information about using tools with extended thinking, see the [extended thinking guide](/docs/en/build-with-claude/extended-thinking#extended-thinking-with-tool-use).&para;<br>&lt;/Note&gt;&para;<br>&para;<br>## 1M token context window&para;<br>&para;<br>Claude Opus 4.6, Sonnet 4.5, and Sonnet 4 support a 1-million token context window. This extended context window allows you to process much larger documents, maintain longer conversations, and work with more extensive codebases.&para;<br>&para;<br>&lt;Note&gt;&para;<br>The 1M token context window is currently in beta for organizations in [usage tier](/docs/en/api/rate-limits) 4 and organizations with custom rate limits. The 1M token context window is only available for Claude Opus 4.6, Sonnet 4.5, and Sonnet 4.&para;<br>&lt;/Note&gt;&para;<br>&para;<br>To use the 1M token context window, include the `context-1m-2025-08-07` [beta header](/docs/en/api/beta-headers) in your API requests:&para;<br>&para;<br>&lt;CodeGroup&gt;&para;<br>&para;<br>```bash cURL&para;<br>curl https://api.anthropic.com/v1/messages \&para;<br>  -H "x-api-key: $ANTHROPIC_API_KEY" \&para;<br>  -H "anthropic-version: 2023-06-01" \&para;<br>  -H "anthropic-beta: context-1m-2025-08-07" \&para;<br>  -H "content-type: application/json" \&para;<br>  -d '{&para;<br>    "model": "claude-opus-4-6",&para;<br>    "max_tokens": 1024,&para;<br>    "messages": [&para;<br>      {"role": "user", "content": "Process this large document..."}&para;<br>    ]&para;<br>  }'&para;<br>```&para;<br>&para;<br>```python Python&para;<br>from anthropic import Anthropic&para;<br>&para;<br>client = Anthropic()&para;<br>&para;<br>response = client.beta.messages.create(&para;<br>    model="claude-opus-4-6",&para;<br>    max_tokens=1024,&para;<br>    messages=[</span><del style="background:#ffe6e6;">&para;<br>        </del><span>{"role": "user", "content": "Process this large document..."}</span><del style="background:#ffe6e6;">&para;<br>    </del><span>],&para;<br>    betas=["context-1m-2025-08-07"]</span><ins style="background:#e6ffe6;">,</ins><span>&para;<br>)&para;<br>```&para;<br>&para;<br>```typescript TypeScript&para;<br>import Anthropic from </span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>@anthropic-ai/sdk</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>;&para;<br>&para;<br>const anthropic = new Anthropic();&para;<br>&para;<br>const msg = await anthropic.beta.messages.create({&para;<br>  model: </span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>claude-opus-4-6</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>,&para;<br>  max_tokens: 1024,&para;<br>  messages: [&para;<br>    { role: </span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>user</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>, content: </span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>Process this large document...</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span> }&para;<br>  ],&para;<br>  betas: [</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>context-1m-2025-08-07</span><del style="background:#ffe6e6;">'</del><ins style="background:#e6ffe6;">"</ins><span>]&para;<br>});&para;<br>```&para;<br>&para;<br>&lt;/CodeGroup&gt;&para;<br>&para;<br>**Important considerations:**&para;<br>- **Beta status:** This is a beta feature subject to change. Features and pricing may be modified or removed in future releases.&para;<br>- **Usage tier requirement:** The 1M token context window is available to organizations in [usage tier](/docs/en/api/rate-limits) 4 and organizations with custom rate limits. Lower tier organizations must advance to usage tier 4 to access this feature.&para;<br>- **Availability:** The 1M token context window is currently available on the Claude API, [Microsoft Foundry](/docs/en/build-with-claude/claude-in-microsoft-foundry), [Amazon Bedrock](/docs/en/build-with-claude/claude-on-amazon-bedrock), and [Google Cloud's Vertex AI](/docs/en/build-with-claude/claude-on-vertex-ai).</span><del style="background:#ffe6e6;"> </del><span>&para;<br>- **Pricing:** Requests exceeding 200K tokens are automatically charged at premium rates (2x input, 1.5x output pricing). See the [pricing documentation](/docs/en/about-claude/pricing#long-context-pricing) for details.&para;<br>- **Rate limits:** Long context requests have dedicated rate limits. See the [rate limits documentation](/docs/en/api/rate-limits#long-context-rate-limits) for details.&para;<br>- **Multimodal considerations:** When processing large numbers of images or pdfs, be aware that the files can vary in token usage. When pairing a large prompt with a large number of images, you may hit [request size limits](/docs/en/api/overview#request-size-limits).&para;<br>&para;<br>## Context awareness in Claude Sonnet 4.5 and Haiku 4.5&para;<br>&para;<br>Claude Sonnet 4.5 and Claude Haiku 4.5 feature **context awareness**. This capability lets these models track their remaining context window (i.e. "token budget") throughout a conversation. This enables Claude to execute tasks and manage context more effectively by understanding how much space it has to work. Claude is trained to use this context precisely, persisting in the task until the very end rather than guessing how many tokens remain. For a model, lacking context awareness is like competing in a cooking show without a clock. Claude 4.5 models change this by explicitly informing the model about its remaining context, so it can take maximum advantage of the available tokens.</span><del style="background:#ffe6e6;"> </del><span>&para;<br>&para;<br>**How it works:**&para;<br>&para;<br>At the start of a conversation, Claude receives information about its total context window:&para;<br>&para;<br>```&para;<br>&lt;budget:token_budget&gt;200000&lt;/budget:token_budget&gt;&para;<br>```&para;<br>&para;<br>The budget is set to 200K tokens (standard), 500K tokens (claude.ai Enterprise), or 1M tokens (beta, for eligible organizations).&para;<br>&para;<br>After each tool call, Claude receives an update on remaining capacity:&para;<br>&para;<br>```&para;<br>&lt;system_warning&gt;Token usage: 35000/200000; 165000 remaining&lt;/system_warning&gt;&para;<br>```&para;<br>&para;<br>This awareness helps Claude determine how much capacity remains for work and enables more effective execution on long-running tasks. Image tokens are included in these budgets.&para;<br>&para;<br>**Benefits:**&para;<br>&para;<br>Context awareness is particularly valuable for:&para;<br>- Long-running agent sessions that require sustained focus&para;<br>- Multi-context-window workflows where state transitions matter&para;<br>- Complex tasks requiring careful token management&para;<br>&para;<br>For prompting guidance on leveraging context awareness, see the [prompting best practices guide](/docs/en/build-with-claude/prompt-engineering/claude-prompting-best-practices#context-awareness-and-multi-window-workflows).&para;<br>&para;<br>## Managing context with compaction&para;<br>&para;<br>If your conversations regularly approach context window limits, [server-side compaction](/docs/en/build-with-claude/compaction) is the recommended approach. Compaction provides server-side summarization that automatically condenses earlier parts of a conversation, enabling long-running conversations beyond context limits with minimal integration work. It is currently available in beta for Claude Opus 4.6.&para;<br>&para;<br>For more specialized needs, [context editing](/docs/en/build-with-claude/context-editing) offers additional strategies:&para;<br>- **Tool result clearing** - Clear old tool results in agentic workflows&para;<br>- **Thinking block clearing** - Manage thinking blocks with extended thinking&para;<br>&para;<br>## Context window management with newer Claude models&para;<br>&para;<br>Newer Claude models (starting with Claude Sonnet 3.7) return a validation error when prompt and output tokens exceed the context window, rather than silently truncating. This change provides more predictable behavior but requires more careful token management.&para;<br>&para;<br>Use the [token counting API](/docs/en/build-with-claude/token-counting) to estimate token usage before sending messages to Claude. This helps you plan and stay within context window limits.&para;<br>&para;<br>See the [model comparison](/docs/en/about-claude/models/overview#latest-models-comparison) table for a list of context window sizes by model.&para;<br>&para;<br>## Next steps&para;<br>&lt;CardGroup cols={2}&gt;&para;<br>  &lt;Card title="Compaction" icon="compress" href="/docs/en/build-with-claude/compaction"&gt;&para;<br>    The recommended strategy for managing context in long-running conversations.&para;<br>  &lt;/Card&gt;&para;<br>  &lt;Card title="Context editing" icon="pen" href="/docs/en/build-with-claude/context-editing"&gt;&para;<br>    Fine-grained strategies like tool result clearing and thinking block clearing.&para;<br>  &lt;/Card&gt;&para;<br>  &lt;Card title="Model comparison table" icon="scales" href="/docs/en/about-claude/models/overview#latest-models-comparison"&gt;&para;<br>    See the model comparison table for a list of context window sizes and input / output token pricing by model.&para;<br>  &lt;/Card&gt;&para;<br>  &lt;Card title="Extended thinking overview" icon="settings" href="/docs/en/build-with-claude/extended-thinking"&gt;&para;<br>    Learn more about how extended thinking works and how to implement it alongside other features such as tool use and prompt caching.&para;<br>  &lt;/Card&gt;&para;<br>&lt;/CardGroup&gt;</span></div>
        </div>

        <h2 style="margin-top: 2rem; margin-bottom: 1rem;">Unified Diff</h2>
        <pre><code>--- a/build-with-claude/context-windows.md
+++ b/build-with-claude/context-windows.md
@@ -68,7 +68,7 @@
   &lt;Step title=&#34;Third Step&#34;&gt;
     - **Input components:** All inputs and the output from the previous turn is carried forward with the exception of the thinking block, which can be dropped now that Claude has completed the entire tool use cycle. The API will automatically strip the thinking block for you if you pass it back, or you can feel free to strip it yourself at this stage. This is also where you would add the next `User` turn.
     - **Output components:** Since there is a new `User` turn outside of the tool use cycle, Claude will generate a new extended thinking block and continue from there.
-    - **Token calculation:** Previous thinking tokens are automatically stripped from context window calculations. All other previous blocks still count as part of the token window, and the thinking block in the current `Assistant` turn counts as part of the context window. 
+    - **Token calculation:** Previous thinking tokens are automatically stripped from context window calculations. All other previous blocks still count as part of the token window, and the thinking block in the current `Assistant` turn counts as part of the context window.
   &lt;/Step&gt;
 &lt;/Steps&gt;
 
@@ -120,25 +120,23 @@
 response = client.beta.messages.create(
     model=&#34;claude-opus-4-6&#34;,
     max_tokens=1024,
-    messages=[
-        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Process this large document...&#34;}
-    ],
-    betas=[&#34;context-1m-2025-08-07&#34;]
+    messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Process this large document...&#34;}],
+    betas=[&#34;context-1m-2025-08-07&#34;],
 )
 ```
 
 ```typescript TypeScript
-import Anthropic from &#39;@anthropic-ai/sdk&#39;;
+import Anthropic from &#34;@anthropic-ai/sdk&#34;;
 
 const anthropic = new Anthropic();
 
 const msg = await anthropic.beta.messages.create({
-  model: &#39;claude-opus-4-6&#39;,
+  model: &#34;claude-opus-4-6&#34;,
   max_tokens: 1024,
   messages: [
-    { role: &#39;user&#39;, content: &#39;Process this large document...&#39; }
+    { role: &#34;user&#34;, content: &#34;Process this large document...&#34; }
   ],
-  betas: [&#39;context-1m-2025-08-07&#39;]
+  betas: [&#34;context-1m-2025-08-07&#34;]
 });
 ```
 
@@ -147,14 +145,14 @@
 **Important considerations:**
 - **Beta status:** This is a beta feature subject to change. Features and pricing may be modified or removed in future releases.
 - **Usage tier requirement:** The 1M token context window is available to organizations in [usage tier](/docs/en/api/rate-limits) 4 and organizations with custom rate limits. Lower tier organizations must advance to usage tier 4 to access this feature.
-- **Availability:** The 1M token context window is currently available on the Claude API, [Microsoft Foundry](/docs/en/build-with-claude/claude-in-microsoft-foundry), [Amazon Bedrock](/docs/en/build-with-claude/claude-on-amazon-bedrock), and [Google Cloud&#39;s Vertex AI](/docs/en/build-with-claude/claude-on-vertex-ai). 
+- **Availability:** The 1M token context window is currently available on the Claude API, [Microsoft Foundry](/docs/en/build-with-claude/claude-in-microsoft-foundry), [Amazon Bedrock](/docs/en/build-with-claude/claude-on-amazon-bedrock), and [Google Cloud&#39;s Vertex AI](/docs/en/build-with-claude/claude-on-vertex-ai).
 - **Pricing:** Requests exceeding 200K tokens are automatically charged at premium rates (2x input, 1.5x output pricing). See the [pricing documentation](/docs/en/about-claude/pricing#long-context-pricing) for details.
 - **Rate limits:** Long context requests have dedicated rate limits. See the [rate limits documentation](/docs/en/api/rate-limits#long-context-rate-limits) for details.
 - **Multimodal considerations:** When processing large numbers of images or pdfs, be aware that the files can vary in token usage. When pairing a large prompt with a large number of images, you may hit [request size limits](/docs/en/api/overview#request-size-limits).
 
 ## Context awareness in Claude Sonnet 4.5 and Haiku 4.5
 
-Claude Sonnet 4.5 and Claude Haiku 4.5 feature **context awareness**. This capability lets these models track their remaining context window (i.e. &#34;token budget&#34;) throughout a conversation. This enables Claude to execute tasks and manage context more effectively by understanding how much space it has to work. Claude is trained to use this context precisely, persisting in the task until the very end rather than guessing how many tokens remain. For a model, lacking context awareness is like competing in a cooking show without a clock. Claude 4.5 models change this by explicitly informing the model about its remaining context, so it can take maximum advantage of the available tokens. 
+Claude Sonnet 4.5 and Claude Haiku 4.5 feature **context awareness**. This capability lets these models track their remaining context window (i.e. &#34;token budget&#34;) throughout a conversation. This enables Claude to execute tasks and manage context more effectively by understanding how much space it has to work. Claude is trained to use this context precisely, persisting in the task until the very end rather than guessing how many tokens remain. For a model, lacking context awareness is like competing in a cooking show without a clock. Claude 4.5 models change this by explicitly informing the model about its remaining context, so it can take maximum advantage of the available tokens.
 
 **How it works:**
 
</code></pre>
    </div>
</body>
</html>